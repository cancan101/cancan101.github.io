<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Rothberg Writes]]></title>
  <link href="https://blog.alexrothberg.com/atom.xml" rel="self"/>
  <link href="https://blog.alexrothberg.com/"/>
  <updated>2022-09-26T13:38:35-04:00</updated>
  <id>https://blog.alexrothberg.com/</id>
  <author>
    <name><![CDATA[Alex Rothberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building an Email Open Tracking System - Part 1]]></title>
    <link href="https://blog.alexrothberg.com/2022/09/14/building-an-email-tracking-system-p1/"/>
    <updated>2022-09-14T00:26:00-04:00</updated>
    <id>https://blog.alexrothberg.com/2022/09/14/building-an-email-tracking-system-p1</id>
    <content type="html"><![CDATA[<p>I develop a system for tracking email &ldquo;opens,&rdquo; which allows the sender of an email message to know when their message is opened (read) by the recipient(s). The system comprises (1) a Chrome extension that adds functionality to the Gmail website, (2) a backend server that tracks the email sends and then the subsequent opens, and (3) an addon for the mobile (Android / iOS) Gmail apps.</p>

<!-- more -->


<p>Want to know when someone opens your emails? How about knowing <em>each</em> time someone opens an email you sent? In the era of iMessage or WhatsApp, where the concept of <a href="https://www.dictionary.com/e/slang/left-on-read/">&ldquo;left on read&rdquo;</a> has entered the common lexicon, tracking the &ldquo;when&rdquo; of your message being read is near and dear to our hearts. Email clients, such as Outlook, have offered have offered <a href="https://en.wikipedia.org/wiki/Email_tracking#Read-receipts">&ldquo;read-receipts&rdquo; (technically MDN &ndash; Message Disposition Notifications)</a> since as far back as 1998 (see <a href="https://datatracker.ietf.org/doc/html/rfc2298">original RFC</a>). Read receipts rely on the recipient&rsquo;s agent (email client) supporting the feature and opting-in.</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/doc-outlook-block-tracking-read-receipt-4.png" title="Dialog asking recipient if they are okay sending a read receipt" ></p>

<p>An alternative that has existed for a while but has recently become more popular is the <a href="https://en.wikipedia.org/wiki/Spy_pixel">&ldquo;tracker pixel&rdquo;</a> (known less affectionately as the &ldquo;spy pixel&rdquo;). This technique involves putting a small (think 1x1) pixel in the HTML body of the email<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. The user&rsquo;s agent will in general<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> load the image when the email message is opened. The server delivering the image is able to record the image access along with the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent">User Agent string</a> and IP of the requesting client<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>.</p>

<p>I set out to build an email open tracking system that employs a tracking pixel approach. I assume the sender uses Gmail / Google Workspace (fka GSuite) and accesses the site through Chrome (later I expand to the mobile Apps). No such requirements are imposed on the recipient. There are of course already a number of Extensions in the Chrome Web Store that offer this functionality such as <a href="https://chrome.google.com/webstore/detail/email-tracker-for-gmail-m/ndnaehgpjlnokgebbaldlmgkapkpjkkb">Mailtrack</a> and <a href="https://chrome.google.com/webstore/detail/yesware-for-chrome/gkjnkapjmjfpipfcccnjbjcbgdnahpjp">Yesware</a>; however, both as a challenge to myself and because of privacy concerns I decided to build my own.</p>

<p>The system boils down to 3 parts:</p>

<ol>
<li>Injecting the HTML for the tracker pixel into the message body on email send</li>
<li>Capturing the &ldquo;view&rdquo; events when the recipient&rsquo;s agent renders the email (i.e. the recipient reads / opens the email)</li>
<li>Presenting those &ldquo;view&rdquo; events to the the sender</li>
</ol>


<p>I built parts (1) and (3) using a <a href="https://developer.chrome.com/docs/extensions/">Chrome extension</a> that modifies the UI of the Gmail webpage. Part (3) was built using a server that served up a minimal pixel that could be tied to the email it was inserted into, while at the same time capturing information on the the client. I later re-built parts (1) and (3) using a <a href="https://developers.google.com/apps-script/add-ons/gmail">Google Apps Script</a> so that I was able to send and see the email open tracking from within the a Gmail mobile app.</p>

<h2>Implementation</h2>

<p>The repos for the components can be found here:</p>

<ol>
<li><a href="https://github.com/cancan101/email-tracking-extension">Chrome Extension</a></li>
<li><a href="https://github.com/cancan101/email-tracking-api">Server</a></li>
<li><a href="https://github.com/cancan101/email-tracking-gmail-addon">Google Apps Script (Gmail Addon)</a></li>
</ol>


<h3>Chrome Extension</h3>

<p>The Chrome extension needs to (1) modify, as needed, the body of email messages as sent, (2) provide a UI for the sender to control when tracking occurs, (3) display when previously tracked messages are opened, and (4) prevent self-tracks (i.e. we want to exclude our <em>own</em> opens of an email from being counted as a view).</p>

<p>I leveraged <a href="https://github.com/lxieyang/chrome-extension-boilerplate-react">this boilerplate project</a> to get started developing my Chrome extension using TypeScript, React, and Webpack. After I had already built for a while, I did realize the project is no longer maintained<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> and there are probably better starters out there now (e.g. <a href="https://github.com/chibat/chrome-extension-typescript-starter">1</a> or <a href="https://github.com/Godiswill/cra-crx-boilerplate">2</a>). Fortunately the project had been updated to use <a href="https://developer.chrome.com/docs/extensions/mv3/intro/">Manifest V3</a>, which while <a href="https://adguard.com/en/blog/how-ad-blocking-is-done.html">more limited in functionality than V2</a> will be needed as <a href="https://developer.chrome.com/docs/extensions/mv3/mv2-sunset/">V2 is being deprecated in 2023</a>.</p>

<p>I then found <a href="https://github.com/KartikTalwar/gmail.js">gmail-js</a>, which is a library purpose built for browser extensions that look to interface with the Gmail page. It offers functions to modify the UI (e.g. inserting buttons on the compose window and modifying the toolbars) as well as hooks for various events (e.g. a message being sent). The library is under active development, well documented, and supports TypeScript. It also has a <a href="https://github.com/josteink/gmailjs-node-boilerplate">great boilerplate project</a> to help get started. When looking for other libraries to help with building a browser extension for Gmail, I also found <a href="https://www.inboxsdk.com/">InboxSDK</a>, a commercial offering similar to gmail-js, but with more of a focus on extending the Gmail UI. Ultimately gmail-js offered enough functionality for extending the UI that InboxSDK was unnecessary.</p>

<p>There are some limitations in gmail-js that I had to work around. The first limitation, stated right in the README, is that &ldquo;Gmail.js does not work as a content-script.&rdquo; <a href="https://developer.chrome.com/docs/extensions/mv3/content_scripts/">From the Chrome docs</a>: &ldquo;Content scripts are files that run in the context of web pages.&rdquo; Basically content scripts are standard (and simplest) way an extension injects JavaScript into a webpage. And unfortunately they don&rsquo;t work with gmail-js<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>. An example workaround is shown in the boilerplate project and involves a <a href="https://github.com/josteink/gmailjs-node-boilerplate/blob/6a194cc21a44f7e0da4e6f8951625f28f9859a17/src/extensionInjector.js">content script that inserts a script tag to then download the <em>actual</em> js code</a>.</p>

<p>The next limitation that I ran into was that even though there was a <a href="https://github.com/KartikTalwar/gmail.js#gmailobservebeforeaction-callback"><code>before</code> send hook</a> for email message, there is no way from within that hook to actually modify the body of the message. From the docs, it seems like this functionality used to exist. Unfortunately, due to changes on with Gmail page, there was no longer a way to modify the message as it is sent. The workaround here is to <a href="https://github.com/KartikTalwar/gmail.js/issues/479#issuecomment-445751298">modify the body of the email message <em>before</em> the Gmail send button is &ldquo;clicked.&rdquo;</a> This can be done either by eagerly inserting the HTML into the body of the message when the compose window is first opened or by &ldquo;proxying&rdquo; the button click with a new button that runs the injection code and <em>then</em> calls the original button (<a href="https://github.com/KartikTalwar/gmail.js#gmaildomcomposecompose_el">using the <code>send</code> method</a>). The proxy button (<a href="https://github.com/KartikTalwar/gmail.js#gmailtoolsadd_compose_buttoncompose_ref-content_html-onclick_action-custom_style_class">created using <code>add_compose_button</code></a>) may be a totally new button or it might just be made to obscure the original button. I opted for the latter which has the benefit of providing an UI fo the user to choose to track vs not track.</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/compose-window.png" title="A new &#34;Track&#34; button is added to the Compose Window" alt="A new &#34;Track&#34; button is added to the Compose Window"></p>

<p>I used a similar approach to extend the &ldquo;Scheduled send&rdquo; button:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/compose-more-menu.png" title="A new &#34;Scheduled Track&#34; button is added to the &#34;More&#34; menu on the Send Button on Compose Window" alt="A new &#34;Scheduled Track&#34; button is added to the &#34;More&#34; menu on the Send Button on Compose Window"></p>

<p>The flow for sending a tracked message now looks like:</p>

<ol>
<li>User opens Compose window (either for a new message or a reply)</li>
<li>An alternative &ldquo;Send&rdquo; button is injected (the red &ldquo;Track&rdquo; button in the image above)</li>
<li>User composes email message normally</li>
<li>Upon clicking the alternative Send button, a tracking image (with a unique <code>id</code>) is injected into the email body (as additional HTML). We may also want to remove / modify old trackers (i.e. by removing HTML).</li>
<li>When the email is eventually sent (some time after (4)), the <code>id</code> for the tracker is sent to our server.</li>
</ol>


<p>The last limitation is that there is no easy way to modify the contents of the prior messages in the compose window. In other words, while I can insert HTML into the current message, I cannot access (i.e. to remove old trackers) from the prior portion of the email thread. The hack here would be to trigger the &ldquo;Show trimmed content&rdquo; button so that the full contents of the thread are downloaded.</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/trimmed-content.png" title="&#34;Show trimmed content&#34; button for downloading the entire thread" alt="&#34;Show trimmed content&#34; button for downloading the entire thread"></p>

<p>Now that we are successfully able to track sent messages using our Chrome extension, I needed away for the sender of the email to see that their messages had been opened. I first added a UI element to the Inbox view that shows when the last email was opened:
<img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/inbox-toolbar.png" title="A tracking info button added to the toolbar on the Inbox" ></p>

<p>and when clicked opens a modal with a list of the most recent message opens. Clicking on any of the them directly opens the relevant thread:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/inbox-tracking-dialog.png" title="A list of recent email opens that can be clicked" ></p>

<p>On the thread view, I also added a button to the toolbar:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/thread-toolbar.png" title="A tracking info button added to the toolbar on the Thread view" ></p>

<p>which when clicked also opens a modal showing all of the views of the current thread along with additional information:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-09-14-building-an-email-tracking-system-p1/thread-tracking-dialog.png" title="Each View of the current Thread is shown along with additional information" ></p>

<p>Another challenge I faced was being able to use React for developing my UI. Unfortunately gmail-js uses jQuery (and has an imperative API) and the Gmail page itself likes to kill, and then rebuild, the toolbar when it wants. I was able to work around this with a combination of <code>MutationObserver</code> and <code>unmountComponentAtNode</code> (<a href="https://github.com/cancan101/email-tracking-extension/blob/838e32ed614ed54767be73dd773c8f73aae54767/src/gmailJsLoader.ts#L164-L202">see code</a>). I was then able to use the <a href="https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi">React Developer Tools</a> to confirm that I was not leaking React Elements.</p>

<h4>Preventing &ldquo;self-Views&rdquo;</h4>

<p>The Chrome extension is also responsible for making sure that the sender doesn&rsquo;t trigger their own View event each time they open their sent email message. There are two ways to handle this: one is to prevent the image from being loaded at all by the sender&rsquo;s agent. The other would be to add logic on the backend to detect and then exclude these loads. I chose the former option, as due to privacy limitations, identifying the sender&rsquo;s loads of the image proved impractical. Ideally if both the sender and recipient are users of my email tracking system, I <em>do</em> want the view event to trigger when the recipient opens the email message, even if they have the extension installed.</p>

<p>The simplest way to block the loads, and the way the other extensions I looked at<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup> accomplished this was using the <code>webRequestBlocking</code> permission as part of the <a href="https://developer.chrome.com/docs/extensions/reference/webRequest/"><code>webRequest</code></a>. Unfortunately thus functionality was <a href="https://gourav.io/blog/block-api-requests-chrome-extension">removed in Manifest V3</a> and replaced with <code>declarativeNetRequest</code>.</p>

<p>Instead of figuring out how to use the new API, I decided to use injected CSS to prevent the image from being downloaded. I also think the rules for <code>declarativeNetRequest</code> are static and thus cannot be customized based upon the logged in user (i.e. the blocking would prevent any images uses by this tracking system from loading, not just those sent by the specific sender). Instead I am able to generate a CSS rule dynamically that excludes images sent by just the sender. Even though <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors#syntax">CSS doesn&rsquo;t seem to have a regex rule</a>, I am able to use a string match to accomplish this. I ended up using a <code>div</code> tag with a <code>background-image</code> rather than a standard <code>img</code> tag. I could not get the CSS to work to prevent the browser from downloading the image file when using the <code>img</code> tag.</p>

<p>This is the CSS rule that gets injected:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='css'><span class='line'><span class="nt">div</span><span class="o">[</span><span class="nt">style</span><span class="o">*=</span><span class="s2">&quot;__BACKEND_URL__/t/__SENDER_ID__/&quot;</span><span class="o">][</span><span class="nt">style</span><span class="o">*=</span><span class="s2">&quot;image.gif&quot;</span><span class="o">][</span><span class="nt">height</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="o">][</span><span class="nt">width</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="o">]</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">display</span><span class="o">:</span> <span class="k">none</span> <span class="cp">!important</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Backend Server</h3>

<p>Stay tuned for Part 2, where I go into more depth on the Backend Server.</p>

<p>The TL; DR, is that the server has a few key end points:</p>

<ol>
<li>Recording an email send (REST / JSON)</li>
<li>Tracking pixel with a route per-sender (e.g. <code>/t/&lt;sender-id&gt;/&lt;tracker-id&gt;/image.gif</code>) returning the smallest possible valid GIF image and returning as quickly as possible</li>
<li>Login endpoints. I use a <a href="https://workos.com/blog/a-guide-to-magic-links">magic link system</a> that sends an email with a login link</li>
<li>Various RESTful endpoints for the UI to display the state of tracking</li>
</ol>


<p>(2) captures the requester&rsquo;s IP and User Agent string and then involves running through a series of rules and calls to third party APIs to gather information on the requester.</p>

<h3>Google Apps Script (Gmail Addon)</h3>

<p>Stay tuned for subsequent posts!</p>

<!-- Keep -->



<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>This same technique is also used <a href="https://en.wikipedia.org/wiki/Web_beacon">on webpages</a> and known a &ldquo;web beacon&rdquo;.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>This assume the email client supports HTML and the user has not disabled loading images (e.g. <a href="https://support.google.com/mail/answer/145919">on Gmail</a>). <a href="https://useplaintext.email/">There are those that lobby for using plaintext email</a>.<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="https://sendloop.com/articles/the-effect-of-gmail-image-proxy-to-email-marketers/">Gmail</a> and other email services proxy the image so the actual IP address and User Agent are not revealed to the server.<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>A number of the dependencies are out of date leading to various <a href="https://github.com/lxieyang/chrome-extension-boilerplate-react/issues/121">security warnings</a> on GitHub. Further <code>npm dedupe</code> should be run to simplify and speed up the install.<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>The limitation is due to how gmail-js subscribes to XHR events and limitations due to security-context / namespaces (<a href="https://github.com/KartikTalwar/gmail.js/issues/274#issuecomment-485352614">see discussion</a>).<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>I found <a href="https://crxextractor.com/">CRX Extractor</a> super helpful in getting access to the source for other extensions in the Chrome Web Store<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Creating a SQLAlchemy Dialect for Airtable]]></title>
    <link href="https://blog.alexrothberg.com/2022/05/18/creating-sqlalchemy-dialect-for-airtable/"/>
    <updated>2022-05-18T12:37:12-04:00</updated>
    <id>https://blog.alexrothberg.com/2022/05/18/creating-sqlalchemy-dialect-for-airtable</id>
    <content type="html"><![CDATA[<p>In this post, I develop a SQLAlchemy Dialect for <a href="https://www.airtable.com/">Airtable</a>. This builds on <a href="https://blog.alexrothberg.com/2022/03/09/running-superset-against-graphql/">my prior work building a Dialect for GraphQL APIs</a>. With an Airtable Dialect, <a href="https://superset.apache.org/">Apache Superset</a> is able to use an Airtable Bases as a datasource. Pandas can load data directly from Airtable using its native SQL reader. The process of building the Dialect allowed me to better understand the Airtable API and data model, which will be helpful when building further services on top of Airtable. These services might include directly exposing the Airtable Base with GraphQL<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> or UI builders backed by Airtable (like <a href="https://docs.retool.com/docs/airtable-integration">Retool</a>). You can <a href="https://github.com/cancan101/airtable-db-api">view the code for the Dialect here</a>. It&rsquo;s also <a href="https://pypi.org/project/sqlalchemy-airtable/">pip installable as <code>sqlalchemy-airtable</code></a>.</p>

<!-- more -->


<h2>Airtable</h2>

<p>Airtable is a no-code / low-code SaaS tool, which <a href="https://en.wikipedia.org/wiki/Airtable">Wikipedia describes as</a>:</p>

<blockquote><p>a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet&hellip;Users can create a database, set up column types, add records, link tables to one another</p></blockquote>

<p>Airtable empowers non-technical (or lazy, technical) users to rapidly build various <a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a> applications such as CRMs, project trackers, product catalogs, etc. The Airtable team has done a great job publishing <a href="https://www.airtable.com/templates">a collection of templates</a> to help users get started. Airtable offers a lot of of great UI elements for CRUD operations that include a very powerful grid view, detail (record-level) views, and &ldquo;<a href="https://support.airtable.com/hc/en-us/articles/206058268-Guide-to-using-the-form-view">forms</a>&rdquo;, which are great for data entry. There are a whole bunch more view types documented <a href="https://support.airtable.com/hc/en-us/articles/360021502314-Getting-started-view-types">here</a>, including Gantt, Kanban, and calendar.</p>

<h3>Data Visualization and Data Analysis</h3>

<p>That being said, performing data visualization and data analysis is somewhat limited within Airtable itself. Over time, the Airtable team has rolled out additional functionality from the <a href="https://support.airtable.com/hc/en-us/articles/203313975-The-summary-bar">summary bar</a>, which offers column-by-column summary stats, to apps (fka blocks), such as the <a href="https://support.airtable.com/hc/en-us/articles/115013249307-Pivot-table-app">pivot table</a> to now more comprehensive <a href="https://support.airtable.com/hc/en-us/sections/360007206734-Data-visualization">data visualization apps</a>. Even with the <a href="https://support.airtable.com/hc/en-us/articles/115013403608-Airtable-apps-overview">app / dashboard</a> concept and the associated <a href="https://airtable.com/marketplace">marketplace</a>, there is still a gap in what an experienced business or data analyst might be looking for and what is offered. Further. Airtable might be just one part of a larger data storage system used at with the organization such that even if a whole bunch more functionality were added, there would still be desire to perform analysis elsewhere. For example, while you might start by building a v1 of your CRM and ticketing systems in Airtable, over time you may find yourself migrating certain systems to purpose-built tools such as Salesforce or Zendesk. Ideally visualizations and analysis can be performed in one place that isn&rsquo;t vertically integrated with the data solution (not to mention in general avoiding vendor lock-in).</p>

<p>Recognizing the desire of its users to use existing BI tools, the Airtable team has written a helpful blog piece, <a href="https://blog.airtable.com/connect-airtable-tableau-google-data-studio-power-bi/">&ldquo;How to connect Airtable to Tableau, Google Data Studio, and Power BI&rdquo;</a>. The prebuilt integrators discussed in the post are helpful if you use one of those BI tools and, in the case of Tableau and Data Studio, actually require an expensive, <a href="https://www.airtable.com/enterprise">enterprise Airtable account</a>.</p>

<p>Railsware built <a href="https://www.coupler.io/sources/airtable">Coupler.io</a> (fka Airtable Importer) to make synchronizing data from Airtable into Google Sheets easy. This opens up using Google Sheets as a BI tool or simple data warehouse. There are a few other tools that attempt to synchronize from Airtable to traditional databases including <a href="https://www.sequin.io/">Sequin</a> (for CRM data) and <a href="https://www.cdata.com/kb/tech/airtable-sync-multiple-databases-ui.rst">CData</a> and others like <a href="https://www.baseql.com/">BaseQL</a> that expose Airtable with a GraphQL API.</p>

<p>I do some of my analysis in Pandas and some in Superset and I wanted a way to connect either of these to Airtable. I am also &ldquo;lazy&rdquo; and looking for a solution that doesn&rsquo;t involved setting up a data warehouse and then running an ETL / synchronization tool.</p>

<h2>Implementation</h2>

<p>I figured I could achieve my objective by building a Shillelagh Adapter which would in effect create a SQLAlchemy Dialect exposing Airtable as a database and allow any Python tool leveraging SQLAlchemy or expecting a DB-API driver to connect.</p>

<p>Fortunately Airtable offers a rich <a href="https://support.airtable.com/hc/en-us/articles/203313985-Public-REST-API">RESTful API</a> for both querying data as well as updating and inserting data. There are a number of client libraries for different languages wrapping this API. The library for <a href="https://github.com/airtable/airtable.js/">JavaScript is &ldquo;official&rdquo;</a> and is maintained by Airtable itself. There is a well-maintained, <a href="https://github.com/gtalarico/pyairtable">unofficial Python library</a> with blocking I/O and a less well maintained <a href="https://github.com/lfparis/airbase">library for Python based on asyncio (non-blocking)</a>. For the purposes of building the DB Dialect, I stick with blocking I/O as that library is better maintained and the underlying libraries used (Shillelagh &ndash;> APSW), <a href="https://github.com/rogerbinns/apsw/issues/325">won&rsquo;t be able to take advantage of non-blocking I/O</a>.</p>

<h3>Metadata</h3>

<p>In order to expose an Airtable Base as a database we need to know the schema. The &ldquo;schema&rdquo; in practice is the table names and for each table, the list of column names and for each column its data types. Fortunately Airtable offers a <a href="https://airtable.com/api/meta">Metadata API</a> that provides this info. Unfortunately, the feature is currently <a href="https://airtable.com/shrWl6yu8cI8C5Dh3">invite only / wait-listed</a>. I applied but have not been lucky enough to be accepted. I document a number of different workarounds for obtaining the metadata <a href="https://github.com/cancan101/airtable-db-api/wiki/Metadata">here</a>. They roughly boil town to a) hardcoding the table list + pulling down some sample data from each table and then guessing the field types, b) allowing the user to pass in a schema definition (can be offline scraped from the API docs page), or c) hitting an endpoint that exposes the metadata (e.g. shared view or base). Option a) is the most seamless / low-friction to the user whereas c) is the most reliable. I ended up starting with option a) but offering partial support for option c).</p>

<p>There are a few gotchas with the guessing approach, the most annoying of which is that the <a href="https://community.airtable.com/t/field-missing-in-api/32355/3">Airtable API entirely omits fields within a record if the value is null or &ldquo;empty&rdquo;</a>. This is particularly problematic when peeking at the first <code>N</code> rows in cases in which new fields were added later on. <a href="https://community.airtable.com/t/sort-on-rest-api-by-createdtime-without-adding-new-column/47478/2">Currently there is no easy way to pull the last <code>N</code> rows</a>. Other fields may be polymorphic, such as those coming from a formula. We err on the side of being conservative, for example assuming that even if we see only integers the field may at some point contain a float.</p>

<h3>Leveraging the API</h3>

<p>The Airtable &ldquo;backing store&rdquo; supports sorting, filtering, and page size limits<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>. Shillelagh by way of APSW by way of <a href="https://www.sqlite.org/vtab.html">SQLite virtual tables</a> is able to <a href="https://www.sqlite.org/vtab.html#omit_constraint_checking_in_bytecode">opt-in to these features</a> to reduce the amount of work that has to be done in memory by SQLite and instead push the sorting and filtering work to the backing store. We just need to translate from <a href="https://shillelagh.readthedocs.io/en/latest/development.html#returning-data">what Shillelagh gives us to what the API expects</a>.</p>

<p>Sorting is relatively easy as it just involves passing the the field names and the desired sort order to the API. Translating filters involves building an <a href="https://support.airtable.com/hc/en-us/articles/203255215-Formula-field-reference">Airtable formula</a> that evaluates as <code>true</code> for the rows we want to return. This was pretty straight forward except for 1) when dealing with <code>IS NULL</code> / <code>IS NOT NULL</code> filters and 2) filtering by <a href="https://support.airtable.com/hc/en-us/articles/360051564873-Record-ID"><code>ID</code></a> or <code>createdTime</code> (these two values are returned on every record). Writing a filter to select for nulls was tricky in that Airtable&rsquo;s concept of null is <code>BLANK()</code> <a href="https://community.airtable.com/t/blank-zero-problem/5662">which is also equal to the number <code>0</code></a>. Filters for <code>ID</code> and <code>createdTime</code> are able to leverage <a href="https://support.airtable.com/hc/en-us/articles/203255215-Formula-field-reference#record_functions">built-in functions of <code>RECORD_ID()</code> and <code>CREATED_TIME()</code></a>.</p>

<h3>Value Encoding Gotchas</h3>

<p>Once a given column&rsquo;s data type is known, decoding is generally pretty straightforward. The gotchas were: parsing relationships (Link records and Lookups), dealing with floats, and dealing with formulas. Relationships are always represented in the API as arrays, regardless of whether &ldquo;allow linking to multiple records&rdquo; is checked. In these cases, the array value needs to be flattened for support in SQLite / APSW. Floats are generally encoded according to the <a href="https://datatracker.ietf.org/doc/html/rfc8259">JSON Spec</a>; however, <a href="https://datatracker.ietf.org/doc/html/rfc8259#section-6">JSON does not support &ldquo;special&rdquo; values like Infinity of NaN</a>. Rather than sending a string value for these, the Airtable API returns a JSON object with a key of <code>specialValue</code> with a corresponding value. Errors resulting from formulas are likewise encoded as an object with a key of <code>error</code> and a corresponding error code (such as <code>#ERROR</code>).</p>

<h2>Database Engine Specs (Superset)</h2>

<p>A &ldquo;database engine spec&rdquo; is required in order to fully utilize Superset when connected to a new datasource type (i.e. our Dialect). Basically the spec tells Superset what features the DB supports. The spec is required for any sort of time series work that requires time-binning. There is a pretty confusing message about &ldquo;Grain&rdquo; and how the options are defined in &ldquo;source code&rdquo;:
<img src="https://blog.alexrothberg.com/images/post_images/2022-05-18-creating-sqlalchemy-dialect-for-airtable/time-grain.png" title="Time Grain (Granularity) for Visualization" ></p>

<p>Ultimately I found <a href="https://preset.io/blog/building-database-connector/#database-engine-specs">this blog post from Preset</a> and this <a href="https://github.com/apache/superset/blob/e69f6292c210d32548308769acd8e670630e9ecd/superset/db_engine_specs/gsheets.py">implementation for the GSheets datasource</a>, which I <a href="https://github.com/cancan101/airtable-db-api/blob/218713cf70b026b731f9dc27a4a3a9ed659291cc/airtabledb/db_engine_specs.py">based mine on</a>. You do also have to <a href="https://github.com/cancan101/airtable-db-api/blob/218713cf70b026b731f9dc27a4a3a9ed659291cc/setup.py#L111-L113">register the spec using an <code>entry_point</code></a>. Once I added this Spec, I was then offered a full set of time grains to choose from. Superset does also now call out the datasource&rsquo;s &ldquo;backend&rdquo;:
<img src="https://blog.alexrothberg.com/images/post_images/2022-05-18-creating-sqlalchemy-dialect-for-airtable/database-backend.png" title="Database Backends" ></p>

<p>I am not currently taking advantage of the UI / parameters (it is all pretty undocumented), but from what I see it looks as if I can tweak the UI to gather specific values from the user.</p>

<h2>JOINing across Tables</h2>

<p>With our Airtable Tables now accessible in SQLAlchemy as database tables, I figured I would try using SQL to perform a <code>JOIN</code> between two Tables in the Base. The <code>JOIN</code> worked, in that the data returned was correct and as expected; however, the performance was terrible and I observed far more calls to the Airtable API than expected and filters were not being passed in. For example if I were joining tables <code>A</code> and <code>B</code> on <code>B.a_id = A.id</code>, I would we see <code>get_data</code> for <code>A</code> called once (with no filter) and then <code>get_data</code> for <code>B</code> called <code>n</code> times (with no filters passed in) where <code>n</code> is the number of rows in <code>A</code>.</p>

<p>I thought this might be due to SQLite&rsquo;s use of a sequential scans rather than an index scan and tried writing a <a href="https://shillelagh.readthedocs.io/en/latest/development.html#estimating-query-cost"><code>get_cost</code> function</a> to provide additional hints to the query planner. While this does cause SQLite to issues filtered queries to the API, cutting down on the amount of data fetched, it does not reduce the number of queries sent to the API. With a <code>get_data</code> function, I see a single call to <code>get_data</code> on <code>A</code> and then <code>n</code> called to <code>get_data</code> for <code>B</code>, but I now see <code>ID=</code> filters being passed in on each call. Using <a href="https://www.sqlite.org/eqp.html"><code>EXPLAIN QUERY PLAN</code></a>, I see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SCAN a VIRTUAL TABLE INDEX 42:[[], []]
</span><span class='line'>SCAN b VIRTUAL TABLE INDEX 42:[[[143, 2]], []]</span></code></pre></td></tr></table></div></figure>


<p>whereas without a <code>get_cost</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>SCAN a VIRTUAL TABLE INDEX 42:[[], []]
</span><span class='line'>SCAN b VIRTUAL TABLE INDEX 42:[[], []]</span></code></pre></td></tr></table></div></figure>


<p>Unfortunately the issue ultimately is that SQLite does not try to hold the tables in memory during a query and instead relies on the Virtual table implementation to do so. Without any caching in our Adapter, this means that our <code>get_data</code> methods is called <code>n</code> times on one of the two tables.</p>

<p>I opened <a href="https://sqlite.org/forum/forumpost/7e2802db01">this discussion</a> on the SQLite forum where the tentative conclusion is to add a <code>SQLITE_INDEX_SCAN_MATERIALIZED</code> to tell SQLite to materialize the whole virtual table (keyed on the used constraints).<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>For example building a <a href="https://www.graphql-mesh.com/docs/handlers/handlers-introduction">&ldquo;Source Handler&rdquo; for GraphQL Mesh</a> for Airtable or a clone of <a href="https://www.baseql.com/">BaseQL</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>While the API supports a <code>LIMIT</code> (page size) concept, it uses cursor-based pagination so specifying <code>OFFSET</code> is not possible.<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>There are some other hints to SQLite that might be important for query optimization such as <a href="https://www.sqlite.org/vtab.html#outputs"><code>SQLITE_INDEX_SCAN_UNIQUE</code></a>:<blockquote><p>the idxFlags field may be set to SQLITE_INDEX_SCAN_UNIQUE to indicate that the virtual table will return only zero or one rows given the input constraints.</p></blockquote><br>However this requires <a href="https://github.com/rogerbinns/apsw/issues/329">some changes to APSW</a>. Additionally indicating the estimated number of rows a query will return, which also <a href="https://github.com/rogerbinns/apsw/issues/188">requires a change to APSW</a>.<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Apache Superset Against a GraphQL API]]></title>
    <link href="https://blog.alexrothberg.com/2022/03/09/running-superset-against-graphql/"/>
    <updated>2022-03-09T17:39:58-05:00</updated>
    <id>https://blog.alexrothberg.com/2022/03/09/running-superset-against-graphql</id>
    <content type="html"><![CDATA[<p>In this blog post I walk through my journey of getting <a href="https://superset.apache.org/">Apache Superset</a> to connect to an arbitrary <a href="https://graphql.org/">GraphQL API</a>. You can <a href="https://github.com/cancan101/graphql-db-api">view the code I wrote to accomplish this here</a>. It&rsquo;s also <a href="https://pypi.org/project/sqlalchemy-graphqlapi/">published to PyPI</a>.</p>

<!-- more -->


<p>I wanted a way for Superset to pull data from a GraphQL API so that I could run this powerful and well used business intelligence (BI) tool on top of the breadth of data sources that can be exposed through GraphQL APIs. Unfortunately Superset wants to connect directly with the underlying database (from its landing page: &ldquo;Superset can connect to <a href="https://superset.apache.org/docs/databases/installing-database-drivers/">any SQL based datasource through SQLAlchemy</a>&rdquo;).</p>

<p>Generally this constraint isn&rsquo;t that restrictive as data engineering best practices are to set up a centralized data warehouse to which various production databases would be synchronized. The database used for the data warehouse (e.g. Amazon Redshift, Google BigQuery) can then be queried directly by Superset.</p>

<p>The alternative, &ldquo;lazy&rdquo; approach is to eschew setting up this secondary data storage system and instead point the BI tool directly at the production database (or, slightly better, at a <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html">read replica of that database</a>). This alternative approach is the route often chosen by small, fast moving, early-stage product engineering teams that either lack the resources or know-how to engage in proper data engineering. Or the engineers just need something &ldquo;quick and dirty&rdquo; to empower the business users to explore and analyze the data themselves and get building ad-hoc reports off the engineering team&rsquo;s plate.</p>

<p>I found myself in in the latter scenario, where I wanted to quickly connect a BI tool to our systems without needing to set up a full data warehouse. There was a twist though. While our primary application&rsquo;s database is Postgres (which is supported by Superset), we augment that data using various external / third-party APIs and expose all of this through a GraphQL API<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. In some cases there are data transformations and business logic in the <a href="https://graphql.org/learn/execution/#root-fields-resolvers">resolvers</a> that I wanted to avoid having to re-implement in the BI tool.</p>

<p>And with that I set out to connect Superset (my BI tool of choice) directly to my GraphQL API. It turns out someone else also had this idea and had kindly filed a <a href="https://github.com/apache/superset/issues/5389">GitHub ticket back in 2018</a>. The ticket unfortunately was closed with:<br>
&nbsp;&nbsp;a) It&rsquo;s possible to extend the built-in connectors. &ldquo;The preferred way is to go through SQLAlchemy when possible.&rdquo;<br>
&nbsp;&nbsp;b) &ldquo;All datasources ready for Superset consumption are single-table&rdquo;.<br>
While (b) could be an issue with GraphQL as GraphQL does allow arbitrary return shapes, if we confine ourselves to top level <a href="https://graphql.org/learn/schema/#lists-and-non-null"><code>List</code>s</a> or <a href="https://relay.dev/graphql/connections.htm"><code>Connection</code>s</a> and flatten related entities, the data looks &ldquo;table-like&rdquo;. (a) boiled down to writing a <a href="https://docs.sqlalchemy.org/en/14/core/engines.html">SQLAlchemy <code>Dialect</code></a> and associated <a href="https://www.python.org/dev/peps/pep-0249/">Python DB-API database driver</a>. A quick skim of both the SQLAlchemy docs and the PEP got me a little worried at the what I might be attempting to bite off.</p>

<p>Before I gave up hope, I decided to take a quick look at <a href="https://superset.apache.org/docs/databases/installing-database-drivers/">the other databases that Superset listed</a> to see if there was one I could perhaps base my approach off of. <a href="https://superset.apache.org/docs/databases/google-sheets">Support for Google Sheets</a> caught my attention and upon digging further, I saw it was based on a library called <a href="https://github.com/betodealmeida/shillelagh"><code>shillelagh</code></a>, whose docs say &ldquo;Shillelagh allows you to easily query non-SQL resources&rdquo; and &ldquo;New adapters are relatively easy to implement. There&rsquo;s a step-by-step tutorial that explains how to create a new adapter to an API.&rdquo; <em>Intriguing</em>. On the surface this was exactly what I was looking for.</p>

<p>I took a look at the <a href="https://shillelagh.readthedocs.io/en/latest/development.html">step-by-step tutorial</a>, which very cleanly walks you through the process of developing a new <code>Adapter</code>. <a href="https://shillelagh.readthedocs.io/en/latest/adapters.html">From the docs</a>: &ldquo;Adapters are plugins that make it possible for Shillelagh to query APIs and other non-SQL resources.&rdquo; Perfect!</p>

<p>The last important detail that I needed to figure out what how to map the arbitrary return shape from the API to something tabular. In particular I wanted the ability to leverage the graph nature of GraphQL and pull in related entities. However the graph may be infinitely deep, so I can&rsquo;t just crawl all related entities. I solved this problem by adding an &ldquo;include&rdquo; concept that specified what related entities should be loaded. this include is specified as a query parameter on the table name. In the case of <a href="https://graphql.org/swapi-graphql">SWAPI</a>, I might want to query the <code>allPeople</code> connection. This would then create columns for all the scalar fields of the <code>Nodes</code> returned. If I wanted to include the fields from the <code>species</code>, the table name would be <code>allPeople?include=species</code>. This would add to the columns the fields on the linked <code>Species</code>. This path could be further traversed as <code>allPeople?include=species,species__homeworld</code>.</p>

<p>The majority of the logic is in the <a href="https://github.com/cancan101/graphql-db-api/blob/main/graphqldb/adapter.py"><code>Adapter</code> class</a> and it roughly boils down to:<br>
1. Taking the table name (exposed on the GraphQL API as a field on <code>query</code>) along with any path traversal instructions and resolving the set of &ldquo;columns&rdquo; (flattened fields) and their types. This leverages <a href="https://graphql.org/learn/introspection/">the introspection functionality of GraphQL APIs</a>.<br>
2. Generating the needed GraphQL query and then mapping the results to a row-like structure.</p>

<p>I also <a href="https://github.com/cancan101/graphql-db-api/blob/main/graphqldb/dialect.py">wrote my own subclass of <code>APSWDialect</code></a>, which while not documented in shillelagh&rsquo;s tutorial was <a href="https://github.com/betodealmeida/shillelagh/blob/a427de0b2d1ac27402d70b8a2ae69468f1f3dcad/src/shillelagh/backends/apsw/dialects/gsheets.py">the approach taken for the <code>gsheets://</code> Dialect</a>. This comes with the benefit of being able to expose the list of tables supported by the <code>Dialect</code>. This class leverages the GraphQL.</p>

<p>Once the <code>graphql</code> <code>Dialect</code> is <a href="https://superset.apache.org/docs/databases/docker-add-drivers">registered with SQLAlchemy</a>, adding the GraphQL API to Superset as a Database is as easy as creating a standard <a href="https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls">Database Url</a> where the <code>host</code> and <code>port</code> (optional) are the host and port of the API and the <code>database</code> portion is the path: <code>graphql://host(:port)/path/to/api</code><sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>.</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-03-09-running-superset-against-graphql/database-config.png" title="Database Config" ></p>

<p>Once the Database is defined, the standard UI for defining Datasets can then be used, The list of &ldquo;tables&rdquo; is auto-populated through introspection of the GraphQL API<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-03-09-running-superset-against-graphql/dataset-add.png" title="Adding a Dataset" ></p>

<p>The columns names and their types can also be sync&#8217;ed from the Dataset:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-03-09-running-superset-against-graphql/dataset-columns.png" title="Dataset Columns" ></p>

<p>Once the Database and the Dataset are defined, the standard Superset tools like charting can be used:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-03-09-running-superset-against-graphql/dataset-charting.png" title="Dataset Charting" ></p>

<p>and exploring:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2022-03-09-running-superset-against-graphql/dataset-explore.png" title="Dataset Exploring" ></p>

<p>Longer term there are a whole bunch of great benefits of using accessing data through a GraphQL API which <a href="https://www.sspaeti.com/blog/analytics-api-with-graphql-the-next-level-of-data-engineering/">this blog post</a> explores further. GraphQL presents a centralized platform / solution for stitching together data from a number of internal (e.g. microservices) and external sources (SaaS tools + third-party APIs). It is also a convenient location to implement authorization and data redaction.</p>

<h1>Appendix</h1>

<h2>Development Experience</h2>

<p>I love to do early stage development work in a <a href="https://jupyter.org/">Jupyter Notebook</a> with the <a href="https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html">autoreload extension</a> (basically hot reloading for Python). However this presented a bit of a problem as both <a href="https://docs.sqlalchemy.org/en/14/core/connections.html#registering-new-dialects">SQLAlchemy</a> and <a href="https://shillelagh.readthedocs.io/en/latest/development.html#informing-shillelagh-of-our-class">shillelagh</a> expected their respective <code>Dialect</code>s and <code>Adapter</code>s to be registered as <a href="https://packaging.python.org/en/latest/specifications/entry-points/">entry points</a>.</p>

<p><a href="https://docs.sqlalchemy.org/en/14/core/connections.html#registering-dialects-in-process">SQLAlchemy&rsquo;s docs provided a way to do this In-Process</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sqlalchemy.dialects</span> <span class="kn">import</span> <span class="n">registry</span>
</span><span class='line'>
</span><span class='line'><span class="n">registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s">&quot;graphql&quot;</span><span class="p">,</span> <span class="s">&quot;__main__&quot;</span><span class="p">,</span> <span class="s">&quot;APSWGraphQLDialect&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>However shillelagh did not (<a href="https://github.com/betodealmeida/shillelagh/issues/181">feature now requested</a>), but I was able to work around the issue:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">pkg_resources</span> <span class="kn">import</span> <span class="n">EntryPoint</span><span class="p">,</span> <span class="n">Distribution</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">shillelagh.backends.apsw</span> <span class="kn">import</span> <span class="n">db</span>
</span><span class='line'>
</span><span class='line'><span class="n">db</span><span class="o">.</span><span class="n">iter_entry_points</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span>
</span><span class='line'>    <span class="n">EntryPoint</span><span class="p">(</span><span class="s">&quot;graphql&quot;</span><span class="p">,</span> <span class="s">&quot;__main__&quot;</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="p">(</span><span class="s">&#39;GraphQLAdapter&#39;</span><span class="p">,),</span> <span class="n">dist</span><span class="o">=</span><span class="n">Distribution</span><span class="p">())</span>
</span><span class='line'><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>We use <a href="https://redis.io/">Redis</a> caching + <a href="https://github.com/graphql/dataloader">DataLoaders</a> to mitigate the cost of accessing these APIs.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>We do still need to specify the scheme (<code>http://</code> vs <code>https://</code>) to use when connecting to the GraphQL API. Options considered: <br>1. Two different Dialects (e.g.<code>graphql://</code> vs <code>graphqls://</code>), <br>2. Different Drivers (e.g. <code>graphql+http://</code> vs <code>graphql+https://</code>) or <br>3. Query parameter attached to the URL (e.g. <code>graphql://host:port/path?is_https=0</code>). <br>I went with (3) as most consistent with other URLs (<a href="https://jdbc.postgresql.org/documentation/head/ssl-client.html">e.g. for Postgres: <code>?sslmode=require</code></a>).<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>In order to set query params on the dataset name, I did have to toggle the <code>Virtual (SQL)</code> radio button, which then let me edit the name as free text:<br><img src="https://blog.alexrothberg.com/images/post_images/2022-03-09-running-superset-against-graphql/dataset-name.png" title="Setting Dataset Name" ><a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pylearn2 MLPs With Custom Data]]></title>
    <link href="https://blog.alexrothberg.com/2014/08/17/pylearn2-mlps-with-custom-data/"/>
    <updated>2014-08-17T23:39:28-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/08/17/pylearn2-mlps-with-custom-data</id>
    <content type="html"><![CDATA[<p>I decided to revisit the <a href="https://blog.alexrothberg.com/2014/05/15/map-recognition/">state map recognition problem</a>, only this time, rather than using an SVM on the Hu moments, I used an <a href="http://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a>. This is not the same as using a &ldquo;deep neural network&rdquo; on the raw image pixels themselves as I am still using using domain specific knowledge to build my features (the Hu moments).</p>

<p>As of my writing this blog post, scikit-learn does not support MLPs (see <a href="https://github.com/scikit-learn/scikit-learn/wiki/GSoC-2014:-Extending-Neural-Networks-Module-for-Scikit-learn">this GSoC for plans to add this feature</a>). Instead I turn to <a href="http://deeplearning.net/software/pylearn2/">pylearn2</a>, the machine learning library from the <a href="http://lisa.iro.umontreal.ca/index_en.html">LISA lab</a>. While pylearn2 is <a href="http://fastml.com/pylearn2-in-practice/">not as easy to use as scikit-learn</a>, <a href="http://deeplearning.net/software/pylearn2/tutorial/notebook_tutorials.html">there are some great tutorials</a> to get you started.</p>

<!-- more -->


<p>I added this line to the bottom of <a href="https://blog.alexrothberg.com/2014/05/15/map-recognition/#notebook-download">my last notebook</a> to dump the Hu moments to a CSV so that I could start working in a fresh notebook:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">items</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">CSV_DATA</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>In my new notebook, I performed the same feature normalization that had for the SVM: taking the log of the Hu moments and then mean centering and std-dev scaling those log-transformed values. I did actually test the MLP classifier without performing these normalization and like the SVM, classification performance degraded.</p>

<p>With the data normalized, I shuffled the data and then split the data into test, validation and training sets (15%, 15%, 70% respectively). The simplest way I found to shuffle the rows of a Pandas DataFrame was: <code>df.ix[np.random.permutation(df.index)]</code>. I initially tried <code>df.apply(np.random.permutation)</code>, which lead to each column being shuffled independently (and my fits being horrible).</p>

<p>I saved the three data sets out to three separate CSV files. I could then use the <a href="http://deeplearning.net/software/pylearn2/library/datasets.html#module-pylearn2.datasets.csv_dataset">CSVDataset</a> in the training YAML to load in the file. I created my YAML file by taking the one used for classifying the MNIST digits in the <a href="http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb">MLP tutorial</a>, modifying the datasets and changing <code>n_classes</code> (output classes) from 10 to 50 and <code>nvis</code> (input features) from 784 to 7. I also had to reduce <code>sparse_init</code> from 15 to 7 after a mildly helpful assertion error was thrown (I am not really sure what this variable does but it has to be &lt;= the number of input features). For good measure I also reduced the size of the first layer from 500 nodes to 50, given that I have far fewer features than the 784 pixels in the MNIST images.</p>

<p>After that I ran the fit and saw that my <code>test_y_misclass</code> went to 0.0 in the final epochs. I was able to plot this convergence using the <a href="http://deeplearning.net/software/pylearn2/library/scripts.html#module-pylearn2.scripts.plot_monitor"><code>plot_monitor</code> script</a>. Since <a href="https://github.com/lisa-lab/pylearn2/issues/1034">I like to have my entire notebook self-contained</a>, I ended up modifying the plot_monitor script to run as a function call from within IPython Notebook (<a href="https://github.com/cancan101/pylearn2/compare/ipython_embed_script">see my fork</a>).</p>

<p>Here is <a href="http://nbviewer.ipython.org/gist/cancan101/ea563e394ea968127e0e">the entire IPython Notebook</a>.</p>

<p>I didn&rsquo;t tinker with regularization, but this topic is well addressed in the pylearn2 tutorials.</p>

<p>In a future post, I plan to use CNN to classify the state map images without having to rely on domain specific knowledge for feature design.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Map Recognition]]></title>
    <link href="https://blog.alexrothberg.com/2014/05/15/map-recognition/"/>
    <updated>2014-05-15T15:27:53-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/05/15/map-recognition</id>
    <content type="html"><![CDATA[<p>I wanted to write a program to identify a map of a US state.</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/texas.png" width="105" height="110" title="Map of Texas" alt="A black and white map of the state of Texas"></p>

<p>To make life a little more challenging, this program had to work even if the maps are rotated and are no longer in the standard <a href="https://en.wikipedia.org/wiki/Map#Orientation_of_maps">&ldquo;North is up&rdquo; orientation</a><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. Further the map images may be off-center and rescaled:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/texas-rotate.png" width="140" height="110" title="Map of Texas Rotated and Translated" alt="A black and white map of the state of Texas that has been rotated so that North is no longer up"></p>

<h4>Data Set</h4>

<p>The first challenge was getting a set of 50 solid-filled maps, one for each of the states. Some Google searching around led to <a href="https://www.50states.com/us.htm">this page</a> which has outlined images for each state map. Those images have not just the outline of the state, but also text with the name of the state, the website&rsquo;s URL, a star showing the state capital and dots indicating what I assume are major cities. In order to standardize the images, I removed the text and filled in the outlines. The fill took care of the star and the dots.</p>

<p><img src="https://www.50states.com/maps/texas.gif" width="152" height="200" title="Original Texas Map" ></p>

<!-- more -->


<p>First some Python to get the list of all state names:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">text</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;http://www.50states.com/us.htm&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
</span><span class='line'><span class="n">doc</span> <span class="o">=</span> <span class="n">lxml</span><span class="o">.</span><span class="n">html</span><span class="o">.</span><span class="n">document_fromstring</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s">&quot;.//ul[@class=&#39;bulletedList&#39;]/li/a&quot;</span><span class="p">):</span>
</span><span class='line'>    <span class="n">url</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;href&quot;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">state_name</span> <span class="o">=</span> <span class="n">posixpath</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">posixpath</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">urlparse</span><span class="o">.</span><span class="n">urlsplit</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'>    <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state_name</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">make_url</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
</span><span class='line'>    <span class="s">&quot;return http://www.50states.com/maps/</span><span class="si">%s</span><span class="s">.gif&quot;</span> <span class="o">%</span> <span class="n">state</span>
</span></code></pre></td></tr></table></div></figure>


<p>Next I tried to open one of these images using OpenCV&rsquo;s <code>imread</code> only to discover that OpenCV <a href="https://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imread">does not handle gifs</a> so I built this utility method to convert the GIF to PNG and then to read them with <code>imread</code>:</p>

<div><script src='https://gist.github.com/80e27feaef8eae0ed921.js'></script>
<noscript><pre><code>from PIL import Image
import requests
import tempfile

def load_gif_url(url):
    with tempfile.NamedTemporaryFile(suffix=&quot;.gif&quot;) as f:
        f.write(requests.get(url).content)
        f.flush()
        img = Image.open(f.name)

    with tempfile.NamedTemporaryFile(suffix=&quot;.png&quot;) as f:
        img.save(f.name)
        f.flush()
        src = cv2.imread(f.name)

    assert src is not None and len(src), &quot;Empty&quot;

    return src
</code></pre></noscript></div>


<p>With the images loaded, I believed the following operations should suffice to get my final, filled-in image:</p>

<ol>
<li>Remove the text heading (strip off some number of rows from the top of each image).</li>
<li>Convert &ldquo;color&rdquo; image to binary image (1=black, 0=white) (this will be <a href="https://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours">required by <code>findContours</code></a>)</li>
<li>Find the &ldquo;contours&rdquo; that bound areas we will fill in (i.e. identify the state outlines).</li>
<li>Convert contours to polygons.</li>
<li>Fill in area bounded by &ldquo;significant&rdquo; polygons.</li>
</ol>


<p>Here when I say significant polygons, I want to make sure the polygon bounds a non-trivial area and is not a stray mark.</p>

<p>I use the following methods to convert image and to display the image using matplotlib:</p>

<div><script src='https://gist.github.com/aca55ff569cb790d7757.js'></script>
<noscript><pre><code>def get_bw(src):
    return cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

def show_color(src):
    plt.imshow(cv2.cvtColor(src, cv2.COLOR_BGR2RGB))
    _ = plt.xticks([]), plt.yticks([])

def show_bw(bw):
    plt.imshow(bw, cmap='gray')
    _ = plt.xticks([]), plt.yticks([])</code></pre></noscript></div>


<p>This is my first attempt at the algorithm:</p>

<div><script src='https://gist.github.com/6b4bf88c74bcacfa9d9f.js'></script>
<noscript><pre><code>def get_state_v0(state):
    url = make_url(state)

    IN = load_gif_url(url)

    #Drop the text at the top
    IN = IN[150:]

    #Convert 3 color channels to 1
    IN_bw = get_bw(IN)
  
    #invert colors (per docs for findContour)
    IMG = 255-IN_bw
    
    out_img = IMG.copy()
    contours, hierarchy = cv2.findContours(out_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    threshold = 0.02

    img = 255*np.ones(IN.shape, dtype=np.uint8)
    for i in xrange(len(contours)):
        cnt = contours[i]
        cnt_len = cv2.arcLength(cnt, True)
        cc = cv2.approxPolyDP(cnt, threshold * cnt_len, True)

        area = cv2.contourArea(cc)

        if cnt_len &gt; 50 and area &gt; 500:
            cv2.drawContours(img, contours, i, (0,0,0),thickness=cv2.cv.CV_FILLED) 

    return img</code></pre></noscript></div>


<p>Which seems to work for most states, but fails for Massachusetts:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/massachusetts-fail.png" title="Failure with Massachusetts" ></p>

<p>This seems to be due to a break somewhere in the outline. A simple <code>dilate</code> with a <code>3x3</code> kernel solved the problem.</p>

<p>The final algorithm is here:</p>

<div><script src='https://gist.github.com/cb6a22142fc5fc4d149c.js'></script>
<noscript><pre><code>def get_state_v1(state):
    url = make_url(state)

    IN = load_gif_url(url)

    #Drop the text at the top
    IN = IN[150:]

    #Convert 3 color channels to 1
    IN_bw = get_bw(IN)
  
    #invert colors (per docs for findContour)
    IMG = 255-IN_bw
    
    # This seems to bre required for Massachusetts
    kernel = np.ones((3,3),np.uint8)
    IMG = cv2.dilate(IMG,kernel,iterations = 1)
    
    out_img = IMG.copy()
    contours, hierarchy = cv2.findContours(out_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    threshold = 0.02

    img = 255*np.ones(IN.shape, dtype=np.uint8)
    for i in xrange(len(contours)):
        cnt = contours[i]
        cnt_len = cv2.arcLength(cnt, True)
        cc = cv2.approxPolyDP(cnt, threshold * cnt_len, True)

        area = cv2.contourArea(cc)

        if cnt_len &gt; 50 and area &gt; 500:
            cv2.drawContours(img, contours, i, (0,0,0),thickness=cv2.cv.CV_FILLED) 

    return img</code></pre></noscript></div>


<p>which works for all states:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/states.png" title="Solid Fills US States" ></p>

<h4>Image Recognition</h4>

<p>I needed some way of comparing two images which may have different rotations, scalings and/or translations. My first thought was to develop a process that normalizes an image to a <a href="https://en.wikipedia.org/wiki/Canonical_form">canonical</a> orientation, scaling and position. I could then take an unlabeled image, and compare its standardized form to my set of canonicalized maps pixel-by-pixel with some distance metric such as MAE or MSE.</p>

<p>How to normalize the image? <a href="https://en.wikipedia.org/wiki/Image_moment">Image moments</a> immediately came to mind. The zeroth moment is the &ldquo;mass&rdquo; of the image (actually the area of the image since I am working with black and white images, where the black pixels have unit mass and the white pixels to have no mass). The area can be used to normalize for image scaling.</p>

<p>The first moment is the center of mass or the centroid of the image. Calculating the centroid is then an <a href="https://en.wikipedia.org/wiki/Centroid#Of_a_finite_set_of_points">arithmetic average of black pixel coordinates</a>. The center of mass is a reference point and that allows me to normalize for translation of the image. Finally, I would like an &ldquo;axis of orientation&rdquo; which I can used to normalize rotation. Wikipedia provides the <a href="https://en.wikipedia.org/wiki/Image_moment#Examples_2">answer</a> but a more complete explanation can be <a href="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT2/node3.html">found here</a>.</p>

<p>At this point I realized that rather than going through the trouble of standardizing the image and then comparing pixel-by-pixel, I could just compare a set of so-called <a href="https://en.wikipedia.org/wiki/Image_moment#Rotation_invariant_moments">&ldquo;rotation invariant moments&rdquo;</a>. These values are invariant for a given image under translation, scale and rotation. OpenCV provides a function to calculate the <a href="https://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#humoments">Hu moments</a>. The Hu moments are the most frequently used, but <a href="http://library.utia.cas.cz/prace/20000033.pdf">Flusser shows there are superior choices</a>. For convenience I will use the seven Hu moments as feature values in a machine learning classifier.</p>

<p>In order to classify the images using the seven feature values, I decided to use <a href="https://scikit-learn.org/">scikit-learn</a>&rsquo;s <a href="https://scikit-learn.org/stable/modules/svm.html#classification">SVM Classifier</a>. I generated training data by taking each of the 50 state images, and then rotating, shifting and scaling them and then getting the Hu moments for the resulting image. I took this set of examples, split 20% out as test data and 80% for training. After observing that each of the seven features takes on very different ranges of values, I <a href="http://www.vis.caltech.edu/~graf/my_papers/proceedings/GraBor01.pdf">made sure to scale my features</a>.</p>

<p>An SVM with a <code>linear</code> kernel resulted in 31% classification accuracy. This is certainly better than the 2% I would get from blind guessing but a long way from what a gifted elementary school student would get after studying US geography. Using an &ldquo;rbf&rdquo; kernel and a default setting for <code>gamma</code> gives an accuracy of 20%. With some tweaking to gamma, I was able to get this up to 99%, but this required a gamma=10000. Having this large a gamma made me question my approach. I reviewed the <a href="https://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#double%20matchShapes(InputArray%20contour1,%20InputArray%20contour2,%20int%20method,%20double%20parameter)">OpenCV method <code>matchShapes</code></a> which also uses the Hu Moments. I noticed that rather than just comparing the Hu Moments, the OpenCV method instead compared the log of the moments.</p>

<p>I changed my features to be the signum&#8217;ed log of the absolute values of the Hu Moments, and then applied standard scaling. This log transform of the Hu moments is pretty common in the literature, see: <a href="http://www.geocomputation.org/2007/7C-Spatial_statistics_3/7C2.pdf">Conley</a><sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>. After doing that my linear kernel SVM was able to achieve a score of 100%. An rbf kernel with default gamma got a score of 99% (looking at the confusion matrix shows some confusion between Utah vs. Arizona and Georgia vs. Missouri, which do look similar):</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/confusion-final.png" title="Confusion Matrix" ></p>

<h4>Blurring</h4>

<p>While not part of the original problem specification, I wanted to see how this classification technique would fare under image blurring. I show below classification scores as a function of increasing amounts of blur. Using a <a href="http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=blur#blur">normalized box filter</a>:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/blur.png" width="400" height="400" title="Blurring" ></p>

<p>and a <a href="https://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur">Gaussian Blur</a>:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-13-map-recognition/gblur.png" width="400" height="400" title="Gaussian Blurring" ></p>

<p>My IPython Notebook is available <a href="https://nbviewer.ipython.org/gist/cancan101/d79cd7e230bf41f1c127">here</a>.</p>

<h3>Extensions</h3>

<p>It would be cool to evaluate this same technique on maps of countries. I have found two source of country maps: <a href="http://www.aneki.com/maps_blank/">this</a> and <a href="http://www.worldatlas.com/sitemap.xml">this</a>. Using the second link, the URLs for the country pages can be extracted using:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">text</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;http://www.worldatlas.com/sitemap.xml&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
</span><span class='line'><span class="n">el</span> <span class="o">=</span> <span class="n">lxml</span><span class="o">.</span><span class="n">etree</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="n">urls</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'><span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">el</span><span class="p">:</span>
</span><span class='line'>    <span class="n">urls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">url</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">urls</span><span class="p">)</span>
</span><span class='line'><span class="n">series</span> <span class="o">=</span> <span class="n">series</span><span class="p">[</span><span class="n">series</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">&quot;outline&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">series</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">&quot;countrys&quot;</span><span class="p">)]</span>
</span><span class='line'><span class="n">series</span> <span class="o">=</span> <span class="n">series</span><span class="p">[</span><span class="o">~</span><span class="n">series</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">&quot;states&quot;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">series</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s">&quot;province&quot;</span><span class="p">)]</span>
</span></code></pre></td></tr></table></div></figure>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>In reality, not only do I need to be concerned with <a href="https://docs.opencv.org/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.html">scale, rotation, and translation</a>, but I also should care about the <a href="https://en.wikipedia.org/wiki/List_of_map_projections">map projection</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>&ldquo;To improve classification rates, the natural logarithm of the absolute value of the seven invariants are used instead of the invariants themselves, because the invariants often have very low absolute values (less than 0.001), so taking the logarithm of the invariants reduces the density of the invariants near the origin. Also, the values of the natural logarithms are then converted into standardized z-scores before classification&rdquo;<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Octopress Paper Cuts]]></title>
    <link href="https://blog.alexrothberg.com/2014/05/14/octopress-paper-cuts/"/>
    <updated>2014-05-14T09:16:22-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/05/14/octopress-paper-cuts</id>
    <content type="html"><![CDATA[<p>Now that I have started writing a few blog posts, I have come across a few <a href="https://en.wikipedia.org/wiki/Paper_cut_bug">&ldquo;paper cuts&rdquo;</a> with the (current) Octopress framework. Some quick background on Octopress: I am currently running the v2.0 version of Octopress which appears to have been released in <a href="http://octopress.org/2011/07/23/octopress-20-surfaces/">July 2011</a>. According to <a href="http://sasheldon.com/blog/2013/07/07/waiting-for-octopress-2-successor/">this blog</a>, the v2.1 version was scrapped in favor of a more significant v3.0. From the <a href="https://twitter.com/octopress">Twitter feed</a> and the <a href="https://github.com/octopress/octopress">GitHub page</a>, v3.0 seems to be in active development but not yet released. That leaves me with using v2.0 for the time being.</p>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/amesbah">@amesbah</a> The current release is a bit neglected right now. The next version is being worked on quite a bit though. <a href="https://t.co/a3cogy8z2O">https://t.co/a3cogy8z2O</a></p>&mdash; Octopress (@octopress) <a href="https://twitter.com/octopress/statuses/466799796465184768">May 15, 2014</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<!-- more -->


<h4>Social Button Alignment</h4>

<p>The first issue that I came across was the vertical alignment of the &ldquo;social buttons&rdquo; on the bottom of each post. You can see the Facebook &ldquo;Like&rdquo; and &ldquo;Share&rdquo; buttons are lower than the Twitter and Google+ buttons:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-14-octopress-paper-cuts/facebook-alignment.png" title="Facebook Alignment Issue" ></p>

<p>which was fixed by following <a href="https://github.com/imathis/octopress/issues/176#issuecomment-6531879">this comment</a>.</p>

<h4>Embedded Gist Rendering</h4>

<p>The next issue was rendering of embedded <a href="https://gist.github.com/">Gists</a>. The rendered Gist look pretty crappy:</p>

<p><img src="https://blog.alexrothberg.com/images/post_images/2014-05-14-octopress-paper-cuts/gist-embed-issues.png" title="Embedded Gist" ></p>

<p>There are a number of people talking about the issue: <a href="http://devspade.com/blog/2013/08/06/fixing-gist-embeds-in-octopress/">here</a> and <a href="https://github.com/imathis/octopress/issues/847">here</a>. <a href="https://github.com/cancan101/cancan101.github.io/commit/d30d95694e5e4915b80e0082fb3ef1caac1f021b">The solution I ended up using was a combination of these plus some of my own CSS.</a></p>

<h4>Meta Keywords and Description on Site Pages</h4>

<p>Currently meta tags for keywords and description are only generated on post pages not on site level pages. A solution is offered <a href="http://qiang.hu/2013/04/octopress-seo-site-keywords-and-description.html">here</a> and a <a href="https://github.com/imathis/octopress/pull/1558">Pull Request</a>.</p>

<h4>Blog in URL Path with Subdomain</h4>

<p>I addressed this issue in a <a href="https://blog.alexrothberg.com/2014/05/13/hello-world/#blog-path">previous post</a>.</p>

<h4>post_url plugin</h4>

<p>In order to use <code>post_url</code> to allow intelligent linking between posts, I had to follow <a href="http://www.drurly.com/blog/2012/06/01/octopress-linking-to-other-posts">this post</a>.</p>

<p>Then I ran into this error message:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Liquid Exception: Tag '{%%20post_url%202014-05-13-hello-world%20%}' was not properly terminated with regexp: /\%}/ in atom.xml</span></code></pre></td></tr></table></div></figure>


<p>Following <a href="https://github.com/davidfstr/rdiscount/issues/75#issuecomment-22607869">this comment</a> fixed that issue.</p>

<p>Since I wanted the ability to use named anchors, I made the following change to the <code>post_url.rb</code> file:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='diff'><span class='line'><span class="gd">--- plugins/post_url.rb.orig    2014-05-14 13:33:36.938495170 -0400</span>
</span><span class='line'><span class="gi">+++ plugins/post_url.rb  2014-05-14 13:30:26.162496398 -0400</span>
</span><span class='line'><span class="gu">@@ -25,8 +25,10 @@</span>
</span><span class='line'>   class PostUrl &lt; Liquid::Tag
</span><span class='line'>     def initialize(tag_name, post, tokens)
</span><span class='line'>       super
</span><span class='line'><span class="gi">+      post, anchor = post.strip.split</span>
</span><span class='line'>       @orig_post = post.strip
</span><span class='line'>       @post = PostComparer.new(@orig_post)
</span><span class='line'><span class="gi">+      @anchor = anchor</span>
</span><span class='line'>     end
</span><span class='line'>
</span><span class='line'>     def render(context)
</span><span class='line'><span class="gu">@@ -34,7 +36,11 @@</span>
</span><span class='line'>
</span><span class='line'>       site.posts.each do |p|
</span><span class='line'>         if @post == p
</span><span class='line'><span class="gd">-          return p.url</span>
</span><span class='line'><span class="gi">+          if @anchor.nil?</span>
</span><span class='line'><span class="gi">+             return p.url</span>
</span><span class='line'><span class="gi">+          else</span>
</span><span class='line'><span class="gi">+             return p.url + &quot;#&quot; + @anchor</span>
</span><span class='line'><span class="gi">+          end</span>
</span><span class='line'>         end
</span><span class='line'>       end
</span></code></pre></td></tr></table></div></figure>


<p>I doubt these will be the last of the paper cuts that I uncover, so stay tuned for more fixes. I am starting to think <a href="http://www.art.net/~hopkins/Don/unix-haters/whinux/your-time.html">this applies</a> to Octopress, but I still enjoy the challenge and the learning experience.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Brad Katsuyamas Fleeting Liquidity: A Simple Analogy for What He Sees]]></title>
    <link href="https://blog.alexrothberg.com/2014/05/13/fleeting-liquidity/"/>
    <updated>2014-05-13T11:00:12-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/05/13/fleeting-liquidity</id>
    <content type="html"><![CDATA[<p>From Michael Lewis <a href="http://www.nytimes.com/2014/04/06/magazine/flash-boys-michael-lewis.html">The Wolf Hunters of Wall Street</a>:</p>

<blockquote><p>Before RBC acquired this supposed state-of-the-art electronic-trading firm, Katsuyamas computers worked as he expected them to. Suddenly they didnt. It used to be that when his trading screens showed 10,000 shares of Intel offered at $22 a share, it meant that he could buy 10,000 shares of Intel for $22 a share. He had only to push a button. By the spring of 2007, however, when he pushed the button to complete a trade, the offers would vanish. In his seven years as a trader, he had always been able to look at the screens on his desk and see the stock market. <strong>Now the market as it appeared on his screens was an illusion</strong>.</p></blockquote>

<p>For someone making $1.5-million-a-year running RBCs  electronic-trading operation, I am surprised how little Brad understands about liquidity. I dont just mean the complicated American equity markets; I mean liquidity in general. A simple analogy illustrates why the market is in fact not an illusion and how a similar experience can happen in other markets.</p>

<!-- more -->


<p>Lets say you are looking to buy airlines tickets to fly your entire extended family, all 30 of them, from Toronto to New York City. The first thing you, as a bargain hunting traveler do, is log onto your favorite travel agent site (be it Expedia, Travelocity, etc). You search for the cheapest fare and get ready to buy. Unfortunately most sites allow you to buy only six tickets at a time. So now you as a savvy shopper open your next four favorite travel agent sites and find the same flight. Assuming you are lucky (and the sites have access to the same fares), you sees the same price on all sites. Now its go time! You successfully buy six tickets on the first site and move on to the second site. Once again you are successful.  Great, 12 tickets down 18 to go.</p>

<p>Now when you try to checkout on the third site, you receive a strange error message: Unable to complete checkout. Please search again. You scratch your head and hit the refresh button. The flight results reload, but the rates have gone up! You decide to move on to the fourth site and see what happens there. The same! You arent able to buy the tickets from your initial search and are once again presented with a higher fare.</p>

<p>So what happened? The airline has a bunch of seats its want to sell. Since its a profit maximizing corporation, it employs revenue management to make as much money as possible on these seats. In the case of my analogy the airline has 12 seats at some low price it is trying to sell. It offers another batch at a higher price. The problem is there are only 12 seats at this price, but the airline doesnt know which travel site its customers will use. So it offers those same 12 seats on <em>all</em> of those sites.  Once those 12 are sold, a new batch of seats is offered at a higher price<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>This is what Brad observers in the American equities market. There are give or take 12 public exchange whose aggregate offer he is observing. However like the airlines, market makers dont know to which exchange liquidity takers (i.e. anyone that sends a market order) will go to. So they have some amount of stock they are comfortable selling, lets say 2,000 shares. So they offer 1,000 shares on each of 12 venues. Now what Brad observes is a total of 12,000 shares offered. He tries to buy all 12,000 shares but ultimately only buys 2,000. This doesnt mean the market is rigged, but rather his notion of liquidity, that is summing up everything he sees, is naive. Its like trying to sum up the tickets offered on all of the travel websites and expecting the airline to sell all those tickets at the same price. This is even more crazy for an airline as the total number of tickets seen might exceeds the number of seats on the flight!</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>The new fare is indicated on the ticket as a different <a href="https://en.wikipedia.org/wiki/Fare_basis_code#Booking_codes">booking code</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World!]]></title>
    <link href="https://blog.alexrothberg.com/2014/05/13/hello-world/"/>
    <updated>2014-05-13T00:56:09-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/05/13/hello-world</id>
    <content type="html"><![CDATA[<p>This is my <a href="https://en.wikipedia.org/wiki/Hello_world_program">inaugural, requisite</a> post on my personal blog.</p>

<p>I have chosen to run my blog with <a href="http://octopress.org/">Octopress</a> rather than WordPress or Blogger. Why? <a href="http://decodize.com/html/moving-from-wordpress-to-octopress/">This blogger</a> and <a href="http://blog.zerilliworks.net/blog/2013/03/16/why-octopress/">this blogger</a> do a good job of explaining. Not only does Octopress allow me to source control the entire blog, but also lets me very easily host the site using <a href="https://pages.github.com/">GitHub Pages</a>.</p>

<!-- more -->


<p>The guide <a href="http://octopress.org/docs/setup/">here</a> does a great job of explaining setting up Octopress on GitHub. While I am using GitHub to host the static content I wanted to use my own custom domain. Following the instructions <a href="http://octopress.org/docs/deploying/github/#custom_domains">here</a> to create a CNAME file and then <a href="http://support.godaddy.com/help/article/680/managing-dns-for-your-domain-names#cnames">adding a simple &ldquo;CNAME Record&rdquo; (an alias) on GoDaddy</a> was all it took for <code>blog.alexrothberg.com</code> to direct to <code>cancan101.github.io</code>.</p>

<h3><a name="blog-path"></a>Simplifying the URL</h3>

<p>The default behavior for Octopress is to put the blog content in the <code>/blog/</code> subdirectory in the URL. Since I am using a subdomain for my namespacing, this proved to be awkward. The URLs started with: <code>http://blog.alexrothberg.com/blog/</code>. <a href="http://xit0.org/2013/04/remove-redundant-slash-blog-prefix-from-octopress-website/">This post</a> offers an easy solution. I also found this <a href="https://github.com/imathis/octopress/issues/464">GitHub Issue</a> discussing the issue. You can see changes I made <a href="https://github.com/cancan101/cancan101.github.io/commit/fa2778c349f7d60a16ed073c17702404d159206b">here</a>. When done with the edits I also had to <a href="http://octopress.org/docs/updating/">run <code>rake update_source</code></a>.</p>

<h3>Helpful Links</h3>

<ul>
<li><a href="http://blog.zerosharp.com/clone-your-octopress-to-blog-from-two-places/">http://blog.zerosharp.com/clone-your-octopress-to-blog-from-two-places/</a></li>
<li><a href="http://learnaholic.me/2012/10/10/deploying-octopress-to-github-pages-and-setting-custom-subdomain-cname/">http://learnaholic.me/2012/10/10/deploying-octopress-to-github-pages-and-setting-custom-subdomain-cname/</a></li>
<li><a href="http://stackoverflow.com/questions/12328828/directory-structure-of-octopress">http://stackoverflow.com/questions/12328828/directory-structure-of-octopress</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
