---
layout: post
title: "Creating a SQLAlchemy Dialect for Airtable"
date: 2022-05-17 12:37:12 -0400
comments: true
categories: business-intelligence no-code
description: I develop a SQLAlchemy Dialect and associated Python DB-API database driver that allows Apache Superset to query data from Airtable.
keywords: business intelligence, airtable, apache superset, SQLAlchemy
published: false
---
In this post, I walk through building a SQLAlchemy Dialect for [Airtable](https://www.airtable.com/). This builds on [my prior work building a Dialect for GraphQL APIs]({% post_url 2022-03-09-running-superset-against-graphql %}). With an Airtable Dialect, [Apache Superset](https://superset.apache.org/) is able to use Airtable bases as a datasource. Building the Dialect allowed me better understand the Airtable API and data model, which will be helpful when building further services on top of Airtable. These services might include directly exposing the Airtable Base with GraphQL[^airtable-graphql] or UI builders backed by Airtable. You can [view the code for the Dialect here](https://github.com/cancan101/airtable-db-api). It's also [published to PyPI](https://pypi.org/project/sqlalchemy-airtable/).
<!-- more -->
[^airtable-graphql]: For example building a [GraphQL Mesh Source Handler](https://www.graphql-mesh.com/docs/handlers/handlers-introduction) for Airtable

## Airtable
Airtable is a no-code / low-code SaaS tool, which [Wikipedia describes as](https://en.wikipedia.org/wiki/Airtable):
>a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet...Users can create a database, set up column types, add records, link tables to one another

Airtable empowers non-technical (or lazy, technical) users to rapidly build various [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) applications such as CRMs, project trackers, product catalogs, etc. The Airtable team has done a great job publishing [a collection of templates](https://www.airtable.com/templates) to help users get started. Airtable offers a lot of of good UI elements for CRUD operations including a very powerful grid view, detail (record-level) views, and "[forms](https://support.airtable.com/hc/en-us/articles/206058268-Guide-to-using-the-form-view)", which are great for data entry. There are a whole bunch more view types documented [here](https://support.airtable.com/hc/en-us/articles/360021502314-Getting-started-view-types), including Gantt, Kanban, and calendar.

### Data Visualization and Data Analysis
That being said, performing data visualization and data analysis is somewhat limited withing the tool. Over time, the Airtable team has rolled out additional functionality from the [summary bar](https://support.airtable.com/hc/en-us/articles/203313975-The-summary-bar), which offers column-by-column summary stats, to apps (fka blocks), such as the [pivot table](https://support.airtable.com/hc/en-us/articles/115013249307-Pivot-table-app) to now more comprehensive [Data visualization apps](https://support.airtable.com/hc/en-us/sections/360007206734-Data-visualization). Even with the [app / dashboard](https://support.airtable.com/hc/en-us/articles/115013403608-Airtable-apps-overview) concept and the associated marketplace, there is still a gap in what an experienced business or data analyst might be looking for and what is offered. Even if a whole bunch more functionality were added, Airtable might be just one part of a larger system data storage used at your organization. For example while you might start by building a v1 of your CRM and Ticketing systems in Airtable, over time you may find yourself migrating certain systems to purposes tools such as Salesforce or Zendesk. Ideally visualizations and analysis are performed can be performed in one place that isn't vertically integrated with the data solution (not to mention in general avoiding vendor lock-in).

Recognizing the desire of their users to use existing BI tools, the Airtable team has written a helpful blog piece, ["How to connect Airtable to Tableau, Google Data Studio, and Power BI"](https://blog.airtable.com/connect-airtable-tableau-google-data-studio-power-bi/). The prebuilt integrators discussed in the post are helpful if you use one of those BI tools and in the case of Tableau and Data Studio actually require an expensive, enterprise Airtable account.

Railsware built [Coupler.io](https://www.coupler.io/sources/airtable) (fka Airtable Importer) to make synchronizing data from Airtable into Google Sheets easy. This opens up using Google Sheets as a BI tool or simple data warehouse. There are a few other tools that attempt to synchronize from Airtable to traditional databases including [Sequin](https://www.sequin.io/) (for CRM data) and [CData](https://www.cdata.com/kb/tech/airtable-sync-multiple-databases-ui.rst) and others like [BaseQL](https://www.baseql.com/) that expose Airtable with a GraphQL API.


I do some of my analysis in Pandas and some in Superset and I wanted a way to connect either of these to Airtable. I am also "lazy" and looking for a solution that doesn't involved setting up a data warehouse and then running an ETL / synchronization tool. I figured I could achieve my objective by building a Shillelagh Adapter which would in effect create a SQLAlchemy Driver exposing Airtable as a database and allow any Python tool leveraging SQLAlchemy or expecting a DB-API driver to connect.

## Implementation
Fortunately Airtable offer a rich [RESTful API](https://support.airtable.com/hc/en-us/articles/203313985-Public-REST-API) for both querying data as well as updating and inserting data. There are a number of client libraries for different languages wrapping this API. The library for [JavaScript is "official"](https://github.com/airtable/airtable.js/) and is maintained by Airtable itself. There is a well-maintained, [unofficial Python library](https://github.com/gtalarico/pyairtable) with blocking I/O and a less well maintained [library for Python based on asyncio (non-blocking)](https://github.com/lfparis/airbase). For the purposes of the building the DB Dialect, I stick with the blocking I/O as the library is better maintained and the underlying libraries used (Shillelagh -> APSW), [won't be able to take advantage of non-blocking I/O](https://github.com/rogerbinns/apsw/issues/325).

### Metadata
Building the XXXX presented some intersting challenges but also affored some interesting YYY.
In order to expose the API as a databsae you need to know the schema ex-anate. Fortunately Airtable offers a  Metadata API that provides this info. Unfortunately, the feature is currenttly invite only and requires applying and then being accepting (which I have not been yet lucky enough). I document a number of different pathways of obtraining the metadata [here](https://github.com/cancan101/airtable-db-api/wiki/Metadata) but they roughly boil town to a) pulling down some sample data and guess the field types, b) allowing the user to pass in a schema defining the data (can be offline scraped from the API docs page), or c) hitting an endpoint that exposes the metadata (e.g. shared view or base). Option a) is the most seamless to the user whereas c) is the most reliable. I ended up starting with option a) but offering partial support for option c). There are a few gotchas with the guessing approach, the most annoying of which is that the Airtable API entirely omits fields within a record if the value is null or "empty". This is particulary problematic when peeking at the first N rows in cases in which new fields were added later on. [CUrrently there is no easy way to pull the last N rows](https://community.airtable.com/t/sort-on-rest-api-by-createdtime-without-adding-new-column/47478/2).

### Leveraging the API
The Airtable "backing store" supports sorting, filtering, and LIMIT (pagination)[^limit-offset]. SXXX by way of APSW by way of SQLite virtual tables are able to opt-in to these features to reduce the amount of work that has to be done on ZZZ inside and instead push that to the backing store. We just need to translate from what SXXX gives us to what Airtable expects. Sorting is relatively easy as it just involves passing the the fields and their sort order. Translating filters involves building an Airtable formula that evalutes as true for the rows we want to return. This was pretty stratight forwad excepet for 1) when dealing with `IS NULL` / `IS NOT NULL` filters and 2) filtering by `ID` or `createdXXX`.
[^limit-offset]: While the API supports `LIMIT`, cursor based pagination is used making specifing `OFFSET` not possible.

mapping sql filters to airtable formula


### Decoding the API
For the most part once a given column's type is known decoding is pretty straightforward. The two and half gotchas were: parsing relationships (Link records and Lookups and ), dealing with floats, and dealing with formulas. Relationships are always represented in the API as arrays, regardless of whether "allow linking to multiple records" is checked and need to be flattened for support in SQLite / APSW. Floats are generally encoded according to the [JSON Spec](https://datatracker.ietf.org/doc/html/rfc8259); however, [JSON does not support "special" values like Infinity of NaN](https://datatracker.ietf.org/doc/html/rfc8259#section-6). Rather than sending a string value for these, the Airtable API returns a JSON object with a key of `specialValue` with a corresponding value. Errors resulting from formulas are likewise encoded as an object with a key of `error` and a corresponding error code (such as `#ERROR`).

## Database Engine Specs (Superset)
A "database engine spec" is required in order to fully utilize Superset when connected to a new datasource type. Basically the spec tells Superset what features the DB supports. The spec is required for any sort of time series work that requires time-binning. There is a pretty confusing message about "Grain" and how the options are defined in "source code":
{% img /images/post_images/2022-05-17-creating-sqlalchemy-dialect-for-airtable/time-grain.png Time Grain (Granularity) for Visualization%}

Ultimately I found [this blog post from Preset](https://preset.io/blog/building-database-connector/#database-engine-specs) and this [implementation for the GSheets datasource](https://github.com/apache/superset/blob/e69f6292c210d32548308769acd8e670630e9ecd/superset/db_engine_specs/gsheets.py) which I [based mine on](https://github.com/cancan101/airtable-db-api/blob/218713cf70b026b731f9dc27a4a3a9ed659291cc/airtabledb/db_engine_specs.py). You do also have to [register the spec using an `entry_point`](https://github.com/cancan101/airtable-db-api/blob/218713cf70b026b731f9dc27a4a3a9ed659291cc/setup.py#L111-L113). Once I added this Spec I then was offered a full set of time grains to choose from. Superset does also now call out the datasource's "backend":
{% img /images/post_images/2022-05-17-creating-sqlalchemy-dialect-for-airtable/database-backend.png Database Backends%}

I am not currently taking advantage of the UI / parameters (it is all pretty undocumented), but from what I see it looks as if I can tweak the UI to gather specific values from the user.


## JOINing across Tables
Given that we are now representing the Airtable Tables as database tables, I figured I would try using SQL to perform a join. While this "works," the performance was terrible. I thought this might be due to index vs sequential scans, and tried writing a [get_cost function](https://shillelagh.readthedocs.io/en/latest/development.html#estimating-query-cost). Firtuanltey the issue ultiamtley is that SQLite does not try to hold the tables in memory so it relies on the Virtual table implemtation to do so. Without any caching in our Dialeact this means that our Dialect is called n times on one of the two tables (where n is the number of rows in the other table). Opened [this discussion](https://sqlite.org/forum/forumpost/7e2802db01) on the SQLite forum where the tentative conlusion is to add a `SQLITE_INDEX_SCAN_MATERIALIZED` to tell SQLite to materialize the whole virtual table (keyed on the used constraints).[^sqlite-perf]

[^sqlite-perf]: There are some other hints to SQLite that might be important for query optimization such as [`SQLITE_INDEX_SCAN_UNIQUE`](https://www.sqlite.org/vtab.html#outputs):<blockquote><p>the idxFlags field may be set to SQLITE_INDEX_SCAN_UNIQUE to indicate that the virtual table will return only zero or one rows given the input constraints.</p></blockquote><br>However this requires [some changes to APSW](https://github.com/rogerbinns/apsw/issues/329). Additionally indicating the estimated number of rows a query will return, which also [requires a change to APSW](https://github.com/rogerbinns/apsw/issues/188).
