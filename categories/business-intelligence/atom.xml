<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Business-intelligence | Rothberg Writes]]></title>
  <link href="https://blog.alexrothberg.com/categories/business-intelligence/atom.xml" rel="self"/>
  <link href="https://blog.alexrothberg.com/"/>
  <updated>2022-11-23T15:30:30-05:00</updated>
  <id>https://blog.alexrothberg.com/</id>
  <author>
    <name><![CDATA[Alex Rothberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Creating a SQLAlchemy Dialect for Airtable]]></title>
    <link href="https://blog.alexrothberg.com/2022/05/18/creating-sqlalchemy-dialect-for-airtable/"/>
    <updated>2022-05-18T12:37:12-04:00</updated>
    <id>https://blog.alexrothberg.com/2022/05/18/creating-sqlalchemy-dialect-for-airtable</id>
    <content type="html"><![CDATA[In this post, I develop a SQLAlchemy Dialect for [Airtable](https://www.airtable.com/). This builds on [my prior work building a Dialect for GraphQL APIs](/2022/03/09/running-superset-against-graphql/). With an Airtable Dialect, [Apache Superset](https://superset.apache.org/) is able to use an Airtable Bases as a datasource. Pandas can load data directly from Airtable using its native SQL reader. The process of building the Dialect allowed me to better understand the Airtable API and data model, which will be helpful when building further services on top of Airtable. These services might include directly exposing the Airtable Base with GraphQL[^airtable-graphql] or UI builders backed by Airtable (like [Retool](https://docs.retool.com/docs/airtable-integration)). You can [view the code for the Dialect here](https://github.com/cancan101/airtable-db-api). It's also [pip installable as `sqlalchemy-airtable`](https://pypi.org/project/sqlalchemy-airtable/).
<!-- more -->
[^airtable-graphql]: For example building a ["Source Handler" for GraphQL Mesh](https://www.graphql-mesh.com/docs/handlers/handlers-introduction) for Airtable or a clone of [BaseQL](https://www.baseql.com/).

## Airtable
Airtable is a no-code / low-code SaaS tool, which [Wikipedia describes as](https://en.wikipedia.org/wiki/Airtable):
>a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet...Users can create a database, set up column types, add records, link tables to one another

Airtable empowers non-technical (or lazy, technical) users to rapidly build various [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) applications such as CRMs, project trackers, product catalogs, etc. The Airtable team has done a great job publishing [a collection of templates](https://www.airtable.com/templates) to help users get started. Airtable offers a lot of of great UI elements for CRUD operations that include a very powerful grid view, detail (record-level) views, and "[forms](https://support.airtable.com/hc/en-us/articles/206058268-Guide-to-using-the-form-view)", which are great for data entry. There are a whole bunch more view types documented [here](https://support.airtable.com/hc/en-us/articles/360021502314-Getting-started-view-types), including Gantt, Kanban, and calendar.

### Data Visualization and Data Analysis
That being said, performing data visualization and data analysis is somewhat limited within Airtable itself. Over time, the Airtable team has rolled out additional functionality from the [summary bar](https://support.airtable.com/hc/en-us/articles/203313975-The-summary-bar), which offers column-by-column summary stats, to apps (fka blocks), such as the [pivot table](https://support.airtable.com/hc/en-us/articles/115013249307-Pivot-table-app) to now more comprehensive [data visualization apps](https://support.airtable.com/hc/en-us/sections/360007206734-Data-visualization). Even with the [app / dashboard](https://support.airtable.com/hc/en-us/articles/115013403608-Airtable-apps-overview) concept and the associated [marketplace](https://airtable.com/marketplace), there is still a gap in what an experienced business or data analyst might be looking for and what is offered. Further. Airtable might be just one part of a larger data storage system used at with the organization such that even if a whole bunch more functionality were added, there would still be desire to perform analysis elsewhere. For example, while you might start by building a v1 of your CRM and ticketing systems in Airtable, over time you may find yourself migrating certain systems to purpose-built tools such as Salesforce or Zendesk. Ideally visualizations and analysis can be performed in one place that isn't vertically integrated with the data solution (not to mention in general avoiding vendor lock-in).

Recognizing the desire of its users to use existing BI tools, the Airtable team has written a helpful blog piece, ["How to connect Airtable to Tableau, Google Data Studio, and Power BI"](https://blog.airtable.com/connect-airtable-tableau-google-data-studio-power-bi/). The prebuilt integrators discussed in the post are helpful if you use one of those BI tools and, in the case of Tableau and Data Studio, actually require an expensive, [enterprise Airtable account](https://www.airtable.com/enterprise).

Railsware built [Coupler.io](https://www.coupler.io/sources/airtable) (fka Airtable Importer) to make synchronizing data from Airtable into Google Sheets easy. This opens up using Google Sheets as a BI tool or simple data warehouse. There are a few other tools that attempt to synchronize from Airtable to traditional databases including [Sequin](https://www.sequin.io/) (for CRM data) and [CData](https://www.cdata.com/kb/tech/airtable-sync-multiple-databases-ui.rst) and others like [BaseQL](https://www.baseql.com/) that expose Airtable with a GraphQL API.

I do some of my analysis in Pandas and some in Superset and I wanted a way to connect either of these to Airtable. I am also "lazy" and looking for a solution that doesn't involved setting up a data warehouse and then running an ETL / synchronization tool.

## Implementation
I figured I could achieve my objective by building a Shillelagh Adapter which would in effect create a SQLAlchemy Dialect exposing Airtable as a database and allow any Python tool leveraging SQLAlchemy or expecting a DB-API driver to connect.

Fortunately Airtable offers a rich [RESTful API](https://support.airtable.com/hc/en-us/articles/203313985-Public-REST-API) for both querying data as well as updating and inserting data. There are a number of client libraries for different languages wrapping this API. The library for [JavaScript is "official"](https://github.com/airtable/airtable.js/) and is maintained by Airtable itself. There is a well-maintained, [unofficial Python library](https://github.com/gtalarico/pyairtable) with blocking I/O and a less well maintained [library for Python based on asyncio (non-blocking)](https://github.com/lfparis/airbase). For the purposes of building the DB Dialect, I stick with blocking I/O as that library is better maintained and the underlying libraries used (Shillelagh -> APSW), [won't be able to take advantage of non-blocking I/O](https://github.com/rogerbinns/apsw/issues/325).

### Metadata
In order to expose an Airtable Base as a database we need to know the schema. The "schema" in practice is the table names and for each table, the list of column names and for each column its data types. Fortunately Airtable offers a [Metadata API](https://airtable.com/api/meta) that provides this info. Unfortunately, the feature is currently [invite only / wait-listed](https://airtable.com/shrWl6yu8cI8C5Dh3). I applied but have not been lucky enough to be accepted. I document a number of different workarounds for obtaining the metadata [here](https://github.com/cancan101/airtable-db-api/wiki/Metadata). They roughly boil town to a) hardcoding the table list + pulling down some sample data from each table and then guessing the field types, b) allowing the user to pass in a schema definition (can be offline scraped from the API docs page), or c) hitting an endpoint that exposes the metadata (e.g. shared view or base). Option a) is the most seamless / low-friction to the user whereas c) is the most reliable. I ended up starting with option a) but offering partial support for option c).

There are a few gotchas with the guessing approach, the most annoying of which is that the [Airtable API entirely omits fields within a record if the value is null or "empty"](https://community.airtable.com/t/field-missing-in-api/32355/3). This is particularly problematic when peeking at the first `N` rows in cases in which new fields were added later on. [Currently there is no easy way to pull the last `N` rows](https://community.airtable.com/t/sort-on-rest-api-by-createdtime-without-adding-new-column/47478/2). Other fields may be polymorphic, such as those coming from a formula. We err on the side of being conservative, for example assuming that even if we see only integers the field may at some point contain a float.

### Leveraging the API
The Airtable "backing store" supports sorting, filtering, and page size limits[^limit-offset]. Shillelagh by way of APSW by way of [SQLite virtual tables](https://www.sqlite.org/vtab.html) is able to [opt-in to these features](https://www.sqlite.org/vtab.html#omit_constraint_checking_in_bytecode) to reduce the amount of work that has to be done in memory by SQLite and instead push the sorting and filtering work to the backing store. We just need to translate from [what Shillelagh gives us to what the API expects](https://shillelagh.readthedocs.io/en/latest/development.html#returning-data).

Sorting is relatively easy as it just involves passing the the field names and the desired sort order to the API. Translating filters involves building an [Airtable formula](https://support.airtable.com/hc/en-us/articles/203255215-Formula-field-reference) that evaluates as `true` for the rows we want to return. This was pretty straight forward except for 1) when dealing with `IS NULL` / `IS NOT NULL` filters and 2) filtering by [`ID`](https://support.airtable.com/hc/en-us/articles/360051564873-Record-ID) or `createdTime` (these two values are returned on every record). Writing a filter to select for nulls was tricky in that Airtable's concept of null is `BLANK()` [which is also equal to the number `0`](https://community.airtable.com/t/blank-zero-problem/5662). Filters for `ID` and `createdTime` are able to leverage [built-in functions of `RECORD_ID()` and `CREATED_TIME()`](https://support.airtable.com/hc/en-us/articles/203255215-Formula-field-reference#record_functions).
[^limit-offset]: While the API supports a `LIMIT` (page size) concept, it uses cursor-based pagination so specifying `OFFSET` is not possible.

### Value Encoding Gotchas
Once a given column's data type is known, decoding is generally pretty straightforward. The gotchas were: parsing relationships (Link records and Lookups), dealing with floats, and dealing with formulas. Relationships are always represented in the API as arrays, regardless of whether "allow linking to multiple records" is checked. In these cases, the array value needs to be flattened for support in SQLite / APSW. Floats are generally encoded according to the [JSON Spec](https://datatracker.ietf.org/doc/html/rfc8259); however, [JSON does not support "special" values like Infinity of NaN](https://datatracker.ietf.org/doc/html/rfc8259#section-6). Rather than sending a string value for these, the Airtable API returns a JSON object with a key of `specialValue` with a corresponding value. Errors resulting from formulas are likewise encoded as an object with a key of `error` and a corresponding error code (such as `#ERROR`).

## Database Engine Specs (Superset)
A "database engine spec" is required in order to fully utilize Superset when connected to a new datasource type (i.e. our Dialect). Basically the spec tells Superset what features the DB supports. The spec is required for any sort of time series work that requires time-binning. There is a pretty confusing message about "Grain" and how the options are defined in "source code":
<img src="/images/post_images/2022-05-18-creating-sqlalchemy-dialect-for-airtable/time-grain.png" title="Time Grain (Granularity) for Visualization" >

Ultimately I found [this blog post from Preset](https://preset.io/blog/building-database-connector/#database-engine-specs) and this [implementation for the GSheets datasource](https://github.com/apache/superset/blob/e69f6292c210d32548308769acd8e670630e9ecd/superset/db_engine_specs/gsheets.py), which I [based mine on](https://github.com/cancan101/airtable-db-api/blob/218713cf70b026b731f9dc27a4a3a9ed659291cc/airtabledb/db_engine_specs.py). You do also have to [register the spec using an `entry_point`](https://github.com/cancan101/airtable-db-api/blob/218713cf70b026b731f9dc27a4a3a9ed659291cc/setup.py#L111-L113). Once I added this Spec, I was then offered a full set of time grains to choose from. Superset does also now call out the datasource's "backend":
<img src="/images/post_images/2022-05-18-creating-sqlalchemy-dialect-for-airtable/database-backend.png" title="Database Backends" >

I am not currently taking advantage of the UI / parameters (it is all pretty undocumented), but from what I see it looks as if I can tweak the UI to gather specific values from the user.


## JOINing across Tables
With our Airtable Tables now accessible in SQLAlchemy as database tables, I figured I would try using SQL to perform a `JOIN` between two Tables in the Base. The `JOIN` worked, in that the data returned was correct and as expected; however, the performance was terrible and I observed far more calls to the Airtable API than expected and filters were not being passed in. For example if I were joining tables `A` and `B` on `B.a_id = A.id`, I would we see `get_data` for `A` called once (with no filter) and then `get_data` for `B` called `n` times (with no filters passed in) where `n` is the number of rows in `A`.

I thought this might be due to SQLite's use of a sequential scans rather than an index scan and tried writing a [`get_cost` function](https://shillelagh.readthedocs.io/en/latest/development.html#estimating-query-cost) to provide additional hints to the query planner. While this does cause SQLite to issues filtered queries to the API, cutting down on the amount of data fetched, it does not reduce the number of queries sent to the API. With a `get_data` function, I see a single call to `get_data` on `A` and then `n` called to `get_data` for `B`, but I now see `ID=` filters being passed in on each call. Using [`EXPLAIN QUERY PLAN`](https://www.sqlite.org/eqp.html), I see:
```
SCAN a VIRTUAL TABLE INDEX 42:[[], []]
SCAN b VIRTUAL TABLE INDEX 42:[[[143, 2]], []]
```
whereas without a `get_cost`:
```
SCAN a VIRTUAL TABLE INDEX 42:[[], []]
SCAN b VIRTUAL TABLE INDEX 42:[[], []]
```

Unfortunately the issue ultimately is that SQLite does not try to hold the tables in memory during a query and instead relies on the Virtual table implementation to do so. Without any caching in our Adapter, this means that our `get_data` methods is called `n` times on one of the two tables.

I opened [this discussion](https://sqlite.org/forum/forumpost/7e2802db01) on the SQLite forum where the tentative conclusion is to add a `SQLITE_INDEX_SCAN_MATERIALIZED` to tell SQLite to materialize the whole virtual table (keyed on the used constraints).[^sqlite-perf]

[^sqlite-perf]: There are some other hints to SQLite that might be important for query optimization such as [`SQLITE_INDEX_SCAN_UNIQUE`](https://www.sqlite.org/vtab.html#outputs):<blockquote><p>the idxFlags field may be set to SQLITE_INDEX_SCAN_UNIQUE to indicate that the virtual table will return only zero or one rows given the input constraints.</p></blockquote><br>However this requires [some changes to APSW](https://github.com/rogerbinns/apsw/issues/329). Additionally indicating the estimated number of rows a query will return, which also [requires a change to APSW](https://github.com/rogerbinns/apsw/issues/188).
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Running Apache Superset Against a GraphQL API]]></title>
    <link href="https://blog.alexrothberg.com/2022/03/09/running-superset-against-graphql/"/>
    <updated>2022-03-09T17:39:58-05:00</updated>
    <id>https://blog.alexrothberg.com/2022/03/09/running-superset-against-graphql</id>
    <content type="html"><![CDATA[In this blog post I walk through my journey of getting [Apache Superset](https://superset.apache.org/) to connect to an arbitrary [GraphQL API](https://graphql.org/). You can [view the code I wrote to accomplish this here](https://github.com/cancan101/graphql-db-api). It's also [published to PyPI](https://pypi.org/project/sqlalchemy-graphqlapi/).
<!-- more -->

I wanted a way for Superset to pull data from a GraphQL API so that I could run this powerful and well used business intelligence (BI) tool on top of the breadth of data sources that can be exposed through GraphQL APIs. Unfortunately Superset wants to connect directly with the underlying database (from its landing page: "Superset can connect to [any SQL based datasource through SQLAlchemy](https://superset.apache.org/docs/databases/installing-database-drivers/)").

Generally this constraint isn't that restrictive as data engineering best practices are to set up a centralized data warehouse to which various production databases would be synchronized. The database used for the data warehouse (e.g. Amazon Redshift, Google BigQuery) can then be queried directly by Superset.

The alternative, "lazy" approach is to eschew setting up this secondary data storage system and instead point the BI tool directly at the production database (or, slightly better, at a [read replica of that database](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html)). This alternative approach is the route often chosen by small, fast moving, early-stage product engineering teams that either lack the resources or know-how to engage in proper data engineering. Or the engineers just need something "quick and dirty" to empower the business users to explore and analyze the data themselves and get building ad-hoc reports off the engineering team's plate.

I found myself in in the latter scenario, where I wanted to quickly connect a BI tool to our systems without needing to set up a full data warehouse. There was a twist though. While our primary application's database is Postgres (which is supported by Superset), we augment that data using various external / third-party APIs and expose all of this through a GraphQL API[^cache-etc]. In some cases there are data transformations and business logic in the [resolvers](https://graphql.org/learn/execution/#root-fields-resolvers) that I wanted to avoid having to re-implement in the BI tool.

[^cache-etc]: We use [Redis](https://redis.io/) caching + [DataLoaders](https://github.com/graphql/dataloader) to mitigate the cost of accessing these APIs.

And with that I set out to connect Superset (my BI tool of choice) directly to my GraphQL API. It turns out someone else also had this idea and had kindly filed a [GitHub ticket back in 2018](https://github.com/apache/superset/issues/5389). The ticket unfortunately was closed with:<br>
&nbsp;&nbsp;a) It's possible to extend the built-in connectors. "The preferred way is to go through SQLAlchemy when possible."<br>
&nbsp;&nbsp;b) "All datasources ready for Superset consumption are single-table".<br>
While (b) could be an issue with GraphQL as GraphQL does allow arbitrary return shapes, if we confine ourselves to top level [`List`s](https://graphql.org/learn/schema/#lists-and-non-null) or [`Connection`s](https://relay.dev/graphql/connections.htm) and flatten related entities, the data looks "table-like". (a) boiled down to writing a [SQLAlchemy `Dialect`](https://docs.sqlalchemy.org/en/14/core/engines.html) and associated [Python DB-API database driver](https://www.python.org/dev/peps/pep-0249/). A quick skim of both the SQLAlchemy docs and the PEP got me a little worried at the what I might be attempting to bite off.

Before I gave up hope, I decided to take a quick look at [the other databases that Superset listed](https://superset.apache.org/docs/databases/installing-database-drivers/) to see if there was one I could perhaps base my approach off of. [Support for Google Sheets](https://superset.apache.org/docs/databases/google-sheets) caught my attention and upon digging further, I saw it was based on a library called [`shillelagh`](https://github.com/betodealmeida/shillelagh), whose docs say "Shillelagh allows you to easily query non-SQL resources" and "New adapters are relatively easy to implement. There's a step-by-step tutorial that explains how to create a new adapter to an API." _Intriguing_. On the surface this was exactly what I was looking for.

I took a look at the [step-by-step tutorial](https://shillelagh.readthedocs.io/en/latest/development.html), which very cleanly walks you through the process of developing a new `Adapter`. [From the docs](https://shillelagh.readthedocs.io/en/latest/adapters.html): "Adapters are plugins that make it possible for Shillelagh to query APIs and other non-SQL resources." Perfect!

The last important detail that I needed to figure out what how to map the arbitrary return shape from the API to something tabular. In particular I wanted the ability to leverage the graph nature of GraphQL and pull in related entities. However the graph may be infinitely deep, so I can't just crawl all related entities. I solved this problem by adding an "include" concept that specified what related entities should be loaded. this include is specified as a query parameter on the table name. In the case of [SWAPI](https://graphql.org/swapi-graphql), I might want to query the `allPeople` connection. This would then create columns for all the scalar fields of the `Nodes` returned. If I wanted to include the fields from the `species`, the table name would be `allPeople?include=species`. This would add to the columns the fields on the linked `Species`. This path could be further traversed as `allPeople?include=species,species__homeworld`.

The majority of the logic is in the [`Adapter` class](https://github.com/cancan101/graphql-db-api/blob/main/graphqldb/adapter.py) and it roughly boils down to:<br>
1. Taking the table name (exposed on the GraphQL API as a field on `query`) along with any path traversal instructions and resolving the set of "columns" (flattened fields) and their types. This leverages [the introspection functionality of GraphQL APIs](https://graphql.org/learn/introspection/).<br>
2. Generating the needed GraphQL query and then mapping the results to a row-like structure.

I also [wrote my own subclass of `APSWDialect`](https://github.com/cancan101/graphql-db-api/blob/main/graphqldb/dialect.py), which while not documented in shillelagh's tutorial was [the approach taken for the `gsheets://` Dialect](https://github.com/betodealmeida/shillelagh/blob/a427de0b2d1ac27402d70b8a2ae69468f1f3dcad/src/shillelagh/backends/apsw/dialects/gsheets.py). This comes with the benefit of being able to expose the list of tables supported by the `Dialect`. This class leverages the GraphQL.

Once the `graphql` `Dialect` is [registered with SQLAlchemy](https://superset.apache.org/docs/databases/docker-add-drivers), adding the GraphQL API to Superset as a Database is as easy as creating a standard [Database Url](https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls) where the `host` and `port` (optional) are the host and port of the API and the `database` portion is the path: `graphql://host(:port)/path/to/api`[^http].

<img src="/images/post_images/2022-03-09-running-superset-against-graphql/database-config.png" title="Database Config" >

[^http]: We do still need to specify the scheme (`http://` vs `https://`) to use when connecting to the GraphQL API. Options considered: <br>1. Two different Dialects (e.g.`graphql://` vs `graphqls://`), <br>2. Different Drivers (e.g. `graphql+http://` vs `graphql+https://`) or <br>3. Query parameter attached to the URL (e.g. `graphql://host:port/path?is_https=0`). <br>I went with (3) as most consistent with other URLs ([e.g. for Postgres: `?sslmode=require`](https://jdbc.postgresql.org/documentation/head/ssl-client.html)).


Once the Database is defined, the standard UI for defining Datasets can then be used, The list of "tables" is auto-populated through introspection of the GraphQL API[^dataset-name]:

<img src="/images/post_images/2022-03-09-running-superset-against-graphql/dataset-add.png" title="Adding a Dataset" >

[^dataset-name]: In order to set query params on the dataset name, I did have to toggle the `Virtual (SQL)` radio button, which then let me edit the name as free text:<br><img src="/images/post_images/2022-03-09-running-superset-against-graphql/dataset-name.png" title="Setting Dataset Name" >

The columns names and their types can also be sync'ed from the Dataset:

<img src="/images/post_images/2022-03-09-running-superset-against-graphql/dataset-columns.png" title="Dataset Columns" >

Once the Database and the Dataset are defined, the standard Superset tools like charting can be used:

<img src="/images/post_images/2022-03-09-running-superset-against-graphql/dataset-charting.png" title="Dataset Charting" >

and exploring:

<img src="/images/post_images/2022-03-09-running-superset-against-graphql/dataset-explore.png" title="Dataset Exploring" >

Longer term there are a whole bunch of great benefits of using accessing data through a GraphQL API which [this blog post](https://www.sspaeti.com/blog/analytics-api-with-graphql-the-next-level-of-data-engineering/) explores further. GraphQL presents a centralized platform / solution for stitching together data from a number of internal (e.g. microservices) and external sources (SaaS tools + third-party APIs). It is also a convenient location to implement authorization and data redaction.

# Appendix
## Development Experience
I love to do early stage development work in a [Jupyter Notebook](https://jupyter.org/) with the [autoreload extension](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html) (basically hot reloading for Python). However this presented a bit of a problem as both [SQLAlchemy](https://docs.sqlalchemy.org/en/14/core/connections.html#registering-new-dialects) and [shillelagh](https://shillelagh.readthedocs.io/en/latest/development.html#informing-shillelagh-of-our-class) expected their respective `Dialect`s and `Adapter`s to be registered as [entry points](https://packaging.python.org/en/latest/specifications/entry-points/).

[SQLAlchemy's docs provided a way to do this In-Process](https://docs.sqlalchemy.org/en/14/core/connections.html#registering-dialects-in-process):
```python
from sqlalchemy.dialects import registry

registry.register("graphql", "__main__", "APSWGraphQLDialect")
```

However shillelagh did not ([feature now requested](https://github.com/betodealmeida/shillelagh/issues/181)), but I was able to work around the issue:
```python
from pkg_resources import EntryPoint, Distribution
from shillelagh.backends.apsw import db

db.iter_entry_points = lambda x: [
    EntryPoint("graphql", "__main__", attrs=('GraphQLAdapter',), dist=Distribution())
]
```
]]></content>
  </entry>
  
</feed>
