<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine-learning | Rothberg Writes]]></title>
  <link href="https://blog.alexrothberg.com/categories/machine-learning/atom.xml" rel="self"/>
  <link href="https://blog.alexrothberg.com/"/>
  <updated>2023-05-30T15:33:51-04:00</updated>
  <id>https://blog.alexrothberg.com/</id>
  <author>
    <name><![CDATA[Alex Rothberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pylearn2 MLPs With Custom Data]]></title>
    <link href="https://blog.alexrothberg.com/2014/08/17/pylearn2-mlps-with-custom-data/"/>
    <updated>2014-08-17T23:39:28-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/08/17/pylearn2-mlps-with-custom-data</id>
    <content type="html"><![CDATA[I decided to revisit the [state map recognition problem](/2014/05/15/map-recognition/), only this time, rather than using an SVM on the Hu moments, I used an [MLP](http://en.wikipedia.org/wiki/Multilayer_perceptron). This is not the same as using a "deep neural network" on the raw image pixels themselves as I am still using using domain specific knowledge to build my features (the Hu moments).

As of my writing this blog post, scikit-learn does not support MLPs (see [this GSoC for plans to add this feature](https://github.com/scikit-learn/scikit-learn/wiki/GSoC-2014:-Extending-Neural-Networks-Module-for-Scikit-learn)). Instead I turn to [pylearn2](http://deeplearning.net/software/pylearn2/), the machine learning library from the [LISA lab](http://lisa.iro.umontreal.ca/index_en.html). While pylearn2 is [not as easy to use as scikit-learn](http://fastml.com/pylearn2-in-practice/), [there are some great tutorials](http://deeplearning.net/software/pylearn2/tutorial/notebook_tutorials.html) to get you started.

<!-- more -->

I added this line to the bottom of [my last notebook](/2014/05/15/map-recognition/#notebook-download) to dump the Hu moments to a CSV so that I could start working in a fresh notebook:

```python
items.to_csv(CSV_DATA, index=False)
```

In my new notebook, I performed the same feature normalization that had for the SVM: taking the log of the Hu moments and then mean centering and std-dev scaling those log-transformed values. I did actually test the MLP classifier without performing these normalization and like the SVM, classification performance degraded.

With the data normalized, I shuffled the data and then split the data into test, validation and training sets (15%, 15%, 70% respectively). The simplest way I found to shuffle the rows of a Pandas DataFrame was: `df.ix[np.random.permutation(df.index)]`. I initially tried `df.apply(np.random.permutation)`, which lead to each column being shuffled independently (and my fits being horrible).

I saved the three data sets out to three separate CSV files. I could then use the [CSVDataset](http://deeplearning.net/software/pylearn2/library/datasets.html#module-pylearn2.datasets.csv_dataset) in the training YAML to load in the file. I created my YAML file by taking the one used for classifying the MNIST digits in the [MLP tutorial](http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb), modifying the datasets and changing `n_classes` (output classes) from 10 to 50 and `nvis` (input features) from 784 to 7. I also had to reduce `sparse_init` from 15 to 7 after a mildly helpful assertion error was thrown (I am not really sure what this variable does but it has to be <= the number of input features). For good measure I also reduced the size of the first layer from 500 nodes to 50, given that I have far fewer features than the 784 pixels in the MNIST images.

After that I ran the fit and saw that my `test_y_misclass` went to 0.0 in the final epochs. I was able to plot this convergence using the [`plot_monitor` script](http://deeplearning.net/software/pylearn2/library/scripts.html#module-pylearn2.scripts.plot_monitor). Since [I like to have my entire notebook self-contained](https://github.com/lisa-lab/pylearn2/issues/1034), I ended up modifying the plot_monitor script to run as a function call from within IPython Notebook ([see my fork](https://github.com/cancan101/pylearn2/compare/ipython_embed_script)).

Here is [the entire IPython Notebook](http://nbviewer.ipython.org/gist/cancan101/ea563e394ea968127e0e).

I didn't tinker with regularization, but this topic is well addressed in the pylearn2 tutorials.

In a future post, I plan to use CNN to classify the state map images without having to rely on domain specific knowledge for feature design.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Map Recognition]]></title>
    <link href="https://blog.alexrothberg.com/2014/05/15/map-recognition/"/>
    <updated>2014-05-15T15:27:53-04:00</updated>
    <id>https://blog.alexrothberg.com/2014/05/15/map-recognition</id>
    <content type="html"><![CDATA[## The Challenge

I wanted to write a program to identify a map of a US state.

<img src="/images/post_images/2014-05-13-map-recognition/texas.png" width="105" height="110" title="Map of Texas" alt="A black and white map of the state of Texas">

To make life a little more challenging, this program had to work even if the maps are rotated and are no longer in the standard ["North is up" orientation](https://en.wikipedia.org/wiki/Map#Orientation_of_maps)[^projection]. Further the map images may be off-center and rescaled:

[^projection]: In reality, not only do I need to be concerned with [scale, rotation, and translation](https://docs.opencv.org/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.html), but I also should care about the [map projection](https://en.wikipedia.org/wiki/List_of_map_projections).

<img src="/images/post_images/2014-05-13-map-recognition/texas-rotate.png" width="140" height="110" title="Map of Texas Rotated and Translated" alt="A black and white map of the state of Texas that has been rotated so that North is no longer up">

##Data Set
The first challenge was getting a set of 50 solid-filled maps, one for each of the states. Some Google searching around led to [this page](https://www.50states.com/us.htm) which has outlined images for each state map. Those images have not just the outline of the state, but also text with the name of the state, the website's URL, a star showing the state capital and dots indicating what I assume are major cities. In order to standardize the images, I removed the text and filled in the outlines. The fill took care of the star and the dots.

<img src="https://www.50states.com/maps/texas.gif" width="152" height="200" title="Original Texas Map" >

<!-- more -->

First some Python to get the list of all state names:

```python
text = requests.get("http://www.50states.com/us.htm").text
doc = lxml.html.document_fromstring(text)

states = []
for a in doc.findall(".//ul[@class='bulletedList']/li/a"):
    url = a.get("href")
    state_name = posixpath.splitext(posixpath.split(urlparse.urlsplit(url).path)[-1])[0]
    states.append(state_name)

def make_url(state):
    "return http://www.50states.com/maps/%s.gif" % state
```

Next I tried to open one of these images using OpenCV's `imread` only to discover that OpenCV [does not handle gifs](https://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imread) so I built this utility method to convert the GIF to PNG and then to read them with `imread`:
<div><script src='https://gist.github.com/80e27feaef8eae0ed921.js'></script>
<noscript><pre><code>from PIL import Image
import requests
import tempfile

def load_gif_url(url):
    with tempfile.NamedTemporaryFile(suffix=&quot;.gif&quot;) as f:
        f.write(requests.get(url).content)
        f.flush()
        img = Image.open(f.name)

    with tempfile.NamedTemporaryFile(suffix=&quot;.png&quot;) as f:
        img.save(f.name)
        f.flush()
        src = cv2.imread(f.name)

    assert src is not None and len(src), &quot;Empty&quot;

    return src
</code></pre></noscript></div>


With the images loaded, I believed the following operations should suffice to get my final, filled-in image:

1.  Remove the text heading (strip off some number of rows from the top of each image).
2.  Convert "color" image to binary image (1=black, 0=white) (this will be [required by `findContours`](https://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours))
3.  Find the "contours" that bound areas we will fill in (i.e. identify the state outlines).
4.  Convert contours to polygons.
5.  Fill in area bounded by "significant" polygons.

Here when I say significant polygons, I want to make sure the polygon bounds a non-trivial area and is not a stray mark.

I use the following methods to convert image and to display the image using matplotlib:
<div><script src='https://gist.github.com/aca55ff569cb790d7757.js'></script>
<noscript><pre><code>def get_bw(src):
    return cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

def show_color(src):
    plt.imshow(cv2.cvtColor(src, cv2.COLOR_BGR2RGB))
    _ = plt.xticks([]), plt.yticks([])

def show_bw(bw):
    plt.imshow(bw, cmap='gray')
    _ = plt.xticks([]), plt.yticks([])</code></pre></noscript></div>


This is my first attempt at the algorithm:
<div><script src='https://gist.github.com/6b4bf88c74bcacfa9d9f.js'></script>
<noscript><pre><code>def get_state_v0(state):
    url = make_url(state)

    IN = load_gif_url(url)

    #Drop the text at the top
    IN = IN[150:]

    #Convert 3 color channels to 1
    IN_bw = get_bw(IN)
  
    #invert colors (per docs for findContour)
    IMG = 255-IN_bw
    
    out_img = IMG.copy()
    contours, hierarchy = cv2.findContours(out_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    threshold = 0.02

    img = 255*np.ones(IN.shape, dtype=np.uint8)
    for i in xrange(len(contours)):
        cnt = contours[i]
        cnt_len = cv2.arcLength(cnt, True)
        cc = cv2.approxPolyDP(cnt, threshold * cnt_len, True)

        area = cv2.contourArea(cc)

        if cnt_len &gt; 50 and area &gt; 500:
            cv2.drawContours(img, contours, i, (0,0,0),thickness=cv2.cv.CV_FILLED) 

    return img</code></pre></noscript></div>


Which seems to work for most states, but fails for Massachusetts:

<img src="/images/post_images/2014-05-13-map-recognition/massachusetts-fail.png" title="Failure with Massachusetts" >

This seems to be due to a break somewhere in the outline. A simple `dilate` with a `3x3` kernel solved the problem.

The final algorithm is here:
<div><script src='https://gist.github.com/cb6a22142fc5fc4d149c.js'></script>
<noscript><pre><code>def get_state_v1(state):
    url = make_url(state)

    IN = load_gif_url(url)

    #Drop the text at the top
    IN = IN[150:]

    #Convert 3 color channels to 1
    IN_bw = get_bw(IN)
  
    #invert colors (per docs for findContour)
    IMG = 255-IN_bw
    
    # This seems to bre required for Massachusetts
    kernel = np.ones((3,3),np.uint8)
    IMG = cv2.dilate(IMG,kernel,iterations = 1)
    
    out_img = IMG.copy()
    contours, hierarchy = cv2.findContours(out_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    threshold = 0.02

    img = 255*np.ones(IN.shape, dtype=np.uint8)
    for i in xrange(len(contours)):
        cnt = contours[i]
        cnt_len = cv2.arcLength(cnt, True)
        cc = cv2.approxPolyDP(cnt, threshold * cnt_len, True)

        area = cv2.contourArea(cc)

        if cnt_len &gt; 50 and area &gt; 500:
            cv2.drawContours(img, contours, i, (0,0,0),thickness=cv2.cv.CV_FILLED) 

    return img</code></pre></noscript></div>


which works for all states:

<img src="/images/post_images/2014-05-13-map-recognition/states.png" title="Solid Fills US States" >

##Image Recognition
I needed some way of comparing two images which may have different rotations, scalings and/or translations. My first thought was to develop a process that normalizes an image to a [canonical](https://en.wikipedia.org/wiki/Canonical_form) orientation, scaling and position. I could then take an unlabeled image, and compare its standardized form to my set of canonicalized maps pixel-by-pixel with some distance metric such as MAE or MSE.

How to normalize the image? [Image moments](https://en.wikipedia.org/wiki/Image_moment) immediately came to mind. The zeroth moment is the "mass" of the image (actually the area of the image since I am working with black and white images, where the black pixels have unit mass and the white pixels to have no mass). The area can be used to normalize for image scaling.

The first moment is the center of mass or the centroid of the image. Calculating the centroid is then an [arithmetic average of black pixel coordinates](https://en.wikipedia.org/wiki/Centroid#Of_a_finite_set_of_points). The center of mass is a reference point and that allows me to normalize for translation of the image. Finally, I would like an "axis of orientation" which I can used to normalize rotation. Wikipedia provides the [answer](https://en.wikipedia.org/wiki/Image_moment#Examples_2) but a more complete explanation can be [found here](https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT2/node3.html).

At this point I realized that rather than going through the trouble of standardizing the image and then comparing pixel-by-pixel, I could just compare a set of so-called ["rotation invariant moments"](https://en.wikipedia.org/wiki/Image_moment#Rotation_invariant_moments). These values are invariant for a given image under translation, scale and rotation. OpenCV provides a function to calculate the [Hu moments](https://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#humoments). The Hu moments are the most frequently used, but [Flusser shows there are superior choices](http://library.utia.cas.cz/prace/20000033.pdf). For convenience I will use the seven Hu moments as feature values in a machine learning classifier.

In order to classify the images using the seven feature values, I decided to use [scikit-learn](https://scikit-learn.org/)'s [SVM Classifier](https://scikit-learn.org/stable/modules/svm.html#classification). I generated training data by taking each of the 50 state images, and then rotating, shifting and scaling them and then getting the Hu moments for the resulting image. I took this set of examples, split 20% out as test data and 80% for training. After observing that each of the seven features takes on very different ranges of values, I [made sure to scale my features](http://www.vis.caltech.edu/~graf/my_papers/proceedings/GraBor01.pdf).

An SVM with a `linear` kernel resulted in 31% classification accuracy. This is certainly better than the 2% I would get from blind guessing but a long way from what a gifted elementary school student would get after studying US geography. Using an "rbf" kernel and a default setting for `gamma` gives an accuracy of 20%. With some tweaking to gamma, I was able to get this up to 99%, but this required a gamma=10000. Having this large a gamma made me question my approach. I reviewed the [OpenCV method `matchShapes`](https://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#double matchShapes(InputArray contour1, InputArray contour2, int method, double parameter\)) which also uses the Hu Moments. I noticed that rather than just comparing the Hu Moments, the OpenCV method instead compared the log of the moments.

I changed my features to be the signum'ed log of the absolute values of the Hu Moments, and then applied standard scaling. This log transform of the Hu moments is pretty common in the literature, see: [Conley](http://www.geocomputation.org/2007/7C-Spatial_statistics_3/7C2.pdf)[^conley]. After doing that my linear kernel SVM was able to achieve a score of 100%. An rbf kernel with default gamma got a score of 99% (looking at the confusion matrix shows some confusion between Utah vs. Arizona and Georgia vs. Missouri, which do look similar):

[^conley]: "To improve classification rates, the natural logarithm of the absolute value of the seven invariants are used instead of the invariants themselves, because the invariants often have very low absolute values (less than 0.001), so taking the logarithm of the invariants reduces the density of the invariants near the origin. Also, the values of the natural logarithms are then converted into standardized z-scores before classification"

<img src="/images/post_images/2014-05-13-map-recognition/confusion-final.png" title="Confusion Matrix" >

##Blurring
While not part of the original problem specification, I wanted to see how this classification technique would fare under image blurring. I show below classification scores as a function of increasing amounts of blur. Using a [normalized box filter](http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=blur#blur):

<img src="/images/post_images/2014-05-13-map-recognition/blur.png" width="400" height="400" title="Blurring" >

and a [Gaussian Blur](https://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur):

<img src="/images/post_images/2014-05-13-map-recognition/gblur.png" width="400" height="400" title="Gaussian Blurring" >

My IPython Notebook is available [here](https://nbviewer.ipython.org/gist/cancan101/d79cd7e230bf41f1c127).

##Extensions
It would be cool to evaluate this same technique on maps of countries. I have found two source of country maps: [this](http://www.aneki.com/maps_blank/) and [this](http://www.worldatlas.com/sitemap.xml). Using the second link, the URLs for the country pages can be extracted using:

```python
text = requests.get("http://www.worldatlas.com/sitemap.xml").text
el = lxml.etree.fromstring(str(text))

urls = []
for url in el:
    urls.append(url[0].text)

series = pd.Series(urls)
series = series[series.str.contains("outline") & series.str.contains("countrys")]
series = series[~series.str.contains("states") & ~series.str.contains("province")]
```
]]></content>
  </entry>
  
</feed>
